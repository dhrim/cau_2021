{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "template_word_sequence_data_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "c33212796a18ce172644b359e00d0f030c7bb01214570476ad60cabee251e7cd"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('py38_tf26': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bTCweaZO2UH"
      },
      "source": [
        "# 문자열 분류 Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU_bnDoaPJZm"
      },
      "source": [
        "# 데이터 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBIEdzkxOmRf"
      },
      "source": [
        "## 설정\n",
        "VOCA_SIZE = 4000 # 어휘 사전의 크기\n",
        "EMBEDDING_SIZE = 64 # 단어를 임베딩한 벡터 크기"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as1wbgBDOmRh"
      },
      "source": [
        "## 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eszUnt2uOmRh"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def load_imdb_data(num_words=VOCA_SIZE):\n",
        "  print('Loading data...')\n",
        "\n",
        "  # 데이터\n",
        "  # (train_x, train_y), (test_x, test_y)\n",
        "  dataset = tf.keras.datasets.imdb.load_data(num_words=VOCA_SIZE)\n",
        "\n",
        "  # 단어와 정수 인덱스를 매핑한 딕셔너리\n",
        "  # word_index = {'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, ... }\n",
        "  word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "\n",
        "  return dataset, word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0SoE8CMOmRo",
        "outputId": "a2861809-dfa4-426f-bdaf-192e344f8ff1"
      },
      "source": [
        "((train_x, train_y), (test_x, test_y)), word_index = load_imdb_data()\n",
        "print(train_x.shape)\n",
        "print(train_y.shape)\n",
        "print(test_x.shape)\n",
        "print(test_y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n",
            "(25000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3TNK6OQOmRq"
      },
      "source": [
        "## 데이터 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nzVh1DAOmRr",
        "outputId": "1fdbc45b-6820-4a2b-e571-f65c84ca850d"
      },
      "source": [
        "print(train_x[0])\n",
        "print(train_y[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWiyZeYQOmRs",
        "outputId": "af3c7f76-7e41-428b-f9cd-cc08bf701b3c"
      },
      "source": [
        "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "# reverse_word_index = {34704: 'fawn', 52009: 'tsukino', 52010: 'nunnery', ... }\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(train_x[0])\n",
        "print(decode_review(train_x[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "<START> this film was just brilliant casting location scenery story direction <UNK> really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> island as myself so i loved the fact there was a real connection with this film the witty <UNK> throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was <UNK> life after all that was <UNK> with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0jzn9_DOmRu"
      },
      "source": [
        "## 각 데이터의 길이"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKcxlhfXOmRx",
        "outputId": "87c1b26d-d1f0-40e9-d429-b1ec19985269"
      },
      "source": [
        "print(len(train_x[0]))\n",
        "print(len(train_x[1]))\n",
        "print(len(train_x[2]))\n",
        "print(len(train_x[3]))\n",
        "print(len(train_x[4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "218\n",
            "189\n",
            "141\n",
            "550\n",
            "147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lUT74fQOmRy"
      },
      "source": [
        "## 데이터 길이 일정하게 하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pp6ZRmO4OmRy",
        "outputId": "c712fac2-5b4a-4a31-ba1a-f26c136b08d8"
      },
      "source": [
        "print(train_x[0])\n",
        "print(len(train_x[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8RQ6MVzOmRz",
        "outputId": "480d0e83-7a8b-4c71-9105-d6e49410c556"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "train_x = sequence.pad_sequences(train_x, maxlen=400, padding='post')\n",
        "test_x = sequence.pad_sequences(test_x, maxlen=400, padding='post')\n",
        "print(train_x.shape)\n",
        "print(test_x.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25000, 400)\n",
            "(25000, 400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwlOhMSWOmR0",
        "outputId": "f0c78a89-b4bc-4089-be70-afa64e7ac77b"
      },
      "source": [
        "print(train_x[0])\n",
        "print(len(train_x[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   1   14   22   16   43  530  973 1622 1385   65  458    2   66 3941\n",
            "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
            "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
            "  172    2 1111   17  546   38   13  447    4  192   50   16    6  147\n",
            " 2025   19   14   22    4 1920    2  469    4   22   71   87   12   16\n",
            "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
            "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
            "    2   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
            "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
            "   52    5   14  407   16   82    2    8    4  107  117    2   15  256\n",
            "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
            "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
            " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476\n",
            "   26  480    5  144   30    2   18   51   36   28  224   92   25  104\n",
            "    4  226   65   16   38 1334   88   12   16  283    5   16    2  113\n",
            "  103   32   15   16    2   19  178   32    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0]\n",
            "400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvhztSSTgbLu"
      },
      "source": [
        "# Template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xob6irGJUg4M"
      },
      "source": [
        "## 단순 RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRPkauOFUkwk",
        "outputId": "52b3a476-f724-4670-9b75-6a375a11cd99"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(400))\n",
        "model.add(Embedding(VOCA_SIZE, EMBEDDING_SIZE))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(250, activation=\"relu\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_x, train_y, batch_size=32, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluation\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss =\", loss)\n",
        "print(\"acc =\", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (None, 400, 64)           256000    \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 400, 64)           256       \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 128)               66048     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 250)               32250     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 250)               1000      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 251       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 355,805\n",
            "Trainable params: 355,177\n",
            "Non-trainable params: 628\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "704/704 [==============================] - 28s 35ms/step - loss: 0.4712 - accuracy: 0.7664 - val_loss: 0.7701 - val_accuracy: 0.6888\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.2658 - accuracy: 0.8991 - val_loss: 0.2882 - val_accuracy: 0.8916\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.1923 - accuracy: 0.9278 - val_loss: 0.3451 - val_accuracy: 0.8700\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.1355 - accuracy: 0.9521 - val_loss: 0.3671 - val_accuracy: 0.8828\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 24s 35ms/step - loss: 0.0968 - accuracy: 0.9665 - val_loss: 0.3865 - val_accuracy: 0.8852\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0701 - accuracy: 0.9748 - val_loss: 0.4206 - val_accuracy: 0.8796\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 24s 35ms/step - loss: 0.0528 - accuracy: 0.9824 - val_loss: 0.5130 - val_accuracy: 0.8760\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0385 - accuracy: 0.9864 - val_loss: 0.6522 - val_accuracy: 0.8796\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0366 - accuracy: 0.9872 - val_loss: 0.6512 - val_accuracy: 0.8748\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 24s 34ms/step - loss: 0.0302 - accuracy: 0.9896 - val_loss: 0.5223 - val_accuracy: 0.8624\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.5595 - accuracy: 0.8633\n",
            "loss = 0.5594831705093384\n",
            "acc = 0.8632799983024597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNCrg9zAYgro"
      },
      "source": [
        "## Word2Vec 사용 RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6II47QKUcX-k"
      },
      "source": [
        "### Word2Vec 사용 위한 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VfNZFVycO08",
        "outputId": "66a50c35-43d6-4d8a-cb43-099d06b02991"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKwmlUz-ccB1"
      },
      "source": [
        "### Word2Vec 읽기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLtquXTuinrz"
      },
      "source": [
        "def download_and_decompress_gzip(http_link, target_link):\n",
        "  \"\"\"Utility function for download & decompress gzip file\"\"\"\n",
        "  import gzip, shutil, os\n",
        "  from urllib.request import urlretrieve\n",
        "\n",
        "  if os.path.exists(target_link): return\n",
        "\n",
        "  file_path, _ = urlretrieve(http_link)\n",
        "\n",
        "  with gzip.open(file_path, 'rb') as gz:\n",
        "    with open(target_link, 'wb') as f_out:\n",
        "        shutil.copyfileobj(gz, f_out)\n",
        "\n",
        "def get_unique_word_set(train_x, test_x):\n",
        "  word_set = set()\n",
        "\n",
        "  for sentence in train_x:\n",
        "    words = decode_review(sentence).split(\" \")\n",
        "    word_set.update(words)\n",
        "\n",
        "  for sentence in test_x:\n",
        "    words = decode_review(sentence).split(\" \")\n",
        "    word_set.update(words)\n",
        "\n",
        "  return word_set    \n",
        "\n",
        "def load_embedding_matrix_from_word2vec(train_x, test_x):\n",
        "\n",
        "  # word2vec 파일 다운로드, 압축 풀기\n",
        "  download_and_decompress_gzip(\n",
        "      \"https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\",\n",
        "      'GoogleNews-vectors-negative300-SLIM.bin',\n",
        "  )\n",
        "\n",
        "  # word2vec 로딩\n",
        "  from gensim.models import KeyedVectors\n",
        "  word2vec = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300-SLIM.bin', binary=True)\n",
        "  \n",
        "  # train_x, test_x 문장들의 단어들 set 구하기\n",
        "  word_set = get_unique_word_set(train_x, test_x)\n",
        "  # word_set = { it, took, kill, rough, power, ... }\n",
        "  # embedding 행렬 생성\n",
        "  embedding_size = word2vec.vector_size\n",
        "  embedding_matrix = np.zeros((VOCA_SIZE, embedding_size))\n",
        "\n",
        "  # word_set에 담긴 단어들에 대한 word2vec 값을 행렬에 설정\n",
        "  for word in word_set:\n",
        "    if word not in word2vec : continue\n",
        "    idx = word_index[word]\n",
        "    # word, idx\n",
        "    # it 12\n",
        "    # took 562\n",
        "    # kill 516\n",
        "    # rough 2683\n",
        "    embedding_matrix[idx] = word2vec[word]\n",
        "    # embedding_matrix[12] = [ 0.05521541 -0.00023065  0.0347889 ... ]\n",
        "      \n",
        "  return embedding_matrix    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MicvSfptnjBC"
      },
      "source": [
        "### embedding_matrix 값 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPvFg1azigfo"
      },
      "source": [
        "embedding_matrix = load_embedding_matrix_from_word2vec(train_x, test_x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgQqQ7IUk6mn",
        "outputId": "12b7e47c-7207-466c-ffe3-e5e270bedbc8"
      },
      "source": [
        "print(embedding_matrix.shape)\n",
        "idx = word_index['it']\n",
        "print(idx)\n",
        "print(embedding_matrix[idx])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4000, 300)\n",
            "12\n",
            "[ 0.05521541 -0.00023065  0.0347889   0.06510951 -0.09702593 -0.04085302\n",
            "  0.03686347 -0.04244884  0.04308717  0.01228782 -0.06064121 -0.1525605\n",
            " -0.04340634 -0.08425936 -0.13404898  0.01484114  0.04308717  0.03095893\n",
            "  0.08872766 -0.09128097  0.02250108  0.05330043  0.05968371 -0.08681267\n",
            "  0.08106772 -0.01484114 -0.03861887 -0.01867111  0.0088967   0.02569272\n",
            " -0.04436383  0.06287535 -0.05298126 -0.05298126  0.04915129 -0.02808645\n",
            " -0.00666255  0.0031318   0.03319308  0.09319596  0.05553458 -0.02537356\n",
            "  0.11106916 -0.03367183 -0.06670532  0.05585374 -0.03431015  0.03798055\n",
            "  0.01938923  0.02345857  0.10213256  0.00428877  0.02018714 -0.02824604\n",
            "  0.03191642  0.01220803  0.00172548 -0.04659798  0.00504678 -0.00172548\n",
            "  0.00841796  0.05776873 -0.05585374 -0.01053242 -0.05744956 -0.0223415\n",
            "  0.00115198  0.00674234 -0.04883213  0.06096037  0.05808789 -0.01364427\n",
            "  0.04117219 -0.05744956 -0.0437255  -0.00630349  0.08362103  0.09000432\n",
            "  0.05617291  0.11170749  0.03446974 -0.10851584  0.07947189  0.00365044\n",
            " -0.10660086 -0.021384    0.03861887  0.15702881  0.002673   -0.03223559\n",
            "  0.00070815  0.02266066 -0.01914985 -0.05681124 -0.05808789 -0.0402147\n",
            " -0.03319308  0.00436856  0.02058609  0.01492093 -0.07979106  0.03143768\n",
            "  0.01891048  0.1053242  -0.09319596  0.01037284 -0.0526621  -0.07979106\n",
            " -0.00574496 -0.12575071 -0.01125054 -0.00706151 -0.01508051 -0.04723631\n",
            "  0.04212968  0.04755547 -0.00466778 -0.00363049 -0.02266066  0.07149279\n",
            " -0.13213399 -0.04723631 -0.05234294  0.09000432 -0.13213399 -0.01755403\n",
            " -0.03079935 -0.02744813  0.02026693 -0.06766282 -0.08138688 -0.00208454\n",
            " -0.05330043 -0.03702305  0.021384   -0.01867111  0.04005511 -0.01404323\n",
            " -0.02601189  0.02489481  0.09319596 -0.12830402  0.07436527 -0.07372694\n",
            "  0.00630349 -0.03861887 -0.09830259 -0.03351225 -0.06223703  0.04500216\n",
            " -0.00897649  0.02329899 -0.14873053  0.01994777 -0.03431015  0.00243363\n",
            " -0.05330043 -0.04915129 -0.04436383 -0.00953503 -0.01811257  0.08489769\n",
            "  0.0347889   0.04244884 -0.0268098  -0.07628025  0.03319308 -0.0347889\n",
            "  0.03431015  0.01188887 -0.03287392  0.06000288 -0.09702593 -0.02010735\n",
            " -0.03462932 -0.00566517  0.08553602 -0.1142608   0.01643696 -0.04723631\n",
            " -0.04978962 -0.06925864 -0.05457709 -0.03766138  0.03686347 -0.04500216\n",
            "  0.01795299  0.05649207  0.09000432  0.04691714 -0.02425648  0.03845929\n",
            " -0.01133033 -0.01356448 -0.04053386 -0.08553602 -0.06032204  0.07149279\n",
            "  0.03079935 -0.09128097  0.01077179  0.0303206  -0.02872478  0.10213256\n",
            "  0.03718263  0.00670245 -0.05010879 -0.00614391  0.07947189 -0.010692\n",
            "  0.03271433  0.09000432 -0.03893804  0.07181195 -0.13596396  0.09447262\n",
            "  0.07181195  0.00335122 -0.04564049  0.032076    0.03814013  0.04340634\n",
            " -0.02361815 -0.00550558  0.06798198 -0.04308717  0.04468299  0.03893804\n",
            "  0.02361815  0.07500359 -0.00216433 -0.02712896  0.04659798 -0.09766426\n",
            "  0.02297983 -0.02250108 -0.09447262 -0.05808789  0.08011022 -0.0115697\n",
            "  0.06223703 -0.11617579 -0.03431015 -0.06287535 -0.04627882 -0.02090526\n",
            "  0.01555926  0.00881691 -0.00022815  0.0125272  -0.06000288 -0.01595821\n",
            "  0.0357464  -0.06574783  0.10340922  0.04340634  0.07436527  0.066067\n",
            " -0.03973595 -0.06191786  0.05840706 -0.01228782 -0.09511095 -0.01938923\n",
            " -0.02154359  0.05393876  0.03526765 -0.05936455 -0.08362103  0.01571884\n",
            "  0.1097925   0.09128097  0.18383861  0.01260699  0.03542723 -0.06415201\n",
            " -0.04053386 -0.06351368 -0.07149279  0.01045263 -0.00412919  0.01188887\n",
            "  0.04787464  0.0437255   0.03367183 -0.06830115  0.0187509  -0.02633105\n",
            "  0.0089366   0.0615987  -0.02154359  0.0963876  -0.10277089  0.00634339\n",
            " -0.08936599  0.02090526  0.07755691  0.01117075  0.03973595 -0.07117362]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgZIn3pHcguj"
      },
      "source": [
        "### 학습 Template"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBwogo8DVHJ6",
        "outputId": "686867ee-d6ad-4010-fe25-762a65ec75dc"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(400))\n",
        "# model.add(Embedding(VOCA_SIZE, embedding_size))\n",
        "model.add(Embedding(embedding_matrix.shape[0],\n",
        "                    embedding_matrix.shape[1],\n",
        "                    input_length=400,\n",
        "                    weights=[embedding_matrix],\n",
        "                    trainable=False\n",
        "                    )\n",
        "          )\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(250))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_x, train_y, batch_size=32, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluation\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss =\", loss)\n",
        "print(\"acc =\", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 400, 300)          1200000   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 400, 300)          1200      \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 128)               186880    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 250)               32250     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 250)               1000      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 251       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 1,421,581\n",
            "Trainable params: 220,481\n",
            "Non-trainable params: 1,201,100\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "704/704 [==============================] - 29s 37ms/step - loss: 0.5678 - accuracy: 0.7010 - val_loss: 0.4250 - val_accuracy: 0.8244\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.5144 - accuracy: 0.7401 - val_loss: 0.4686 - val_accuracy: 0.7628\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.3746 - accuracy: 0.8371 - val_loss: 0.3499 - val_accuracy: 0.8556\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.3043 - accuracy: 0.8748 - val_loss: 0.3353 - val_accuracy: 0.8632\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.2511 - accuracy: 0.9012 - val_loss: 0.3974 - val_accuracy: 0.8580\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.2209 - accuracy: 0.9127 - val_loss: 0.3953 - val_accuracy: 0.8480\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.1940 - accuracy: 0.9256 - val_loss: 0.3470 - val_accuracy: 0.8776\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 25s 35ms/step - loss: 0.1696 - accuracy: 0.9355 - val_loss: 0.3811 - val_accuracy: 0.8716\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 25s 36ms/step - loss: 0.1416 - accuracy: 0.9470 - val_loss: 0.3869 - val_accuracy: 0.8732\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 25s 35ms/step - loss: 0.1166 - accuracy: 0.9567 - val_loss: 0.4463 - val_accuracy: 0.8620\n",
            "782/782 [==============================] - 11s 14ms/step - loss: 0.4277 - accuracy: 0.8634\n",
            "loss = 0.427738755941391\n",
            "acc = 0.8633999824523926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ViwQ0ErWRTs"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTJY2WqxWTRW",
        "outputId": "40dade1e-0c6b-4ee6-e3ff-6ef3435b97d5"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Input(400))\n",
        "model.add(Embedding(VOCA_SIZE, EMBEDDING_SIZE))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv1D(250, 3, padding=\"same\"))\n",
        "model.add(Conv1D(250, 3, padding=\"same\"))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(250))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation('sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(train_x, train_y, batch_size=32, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Evaluation\n",
        "loss, acc = model.evaluate(test_x, test_y)\n",
        "print(\"loss =\", loss)\n",
        "print(\"acc =\", acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 400, 64)           256000    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 400, 64)           256       \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 400, 250)          48250     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 400, 250)          187750    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 250)               62750     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 250)               1000      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 251       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 556,257\n",
            "Trainable params: 555,629\n",
            "Non-trainable params: 628\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "704/704 [==============================] - 20s 9ms/step - loss: 0.3902 - accuracy: 0.8176 - val_loss: 0.3369 - val_accuracy: 0.8560\n",
            "Epoch 2/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.2189 - accuracy: 0.9119 - val_loss: 0.7074 - val_accuracy: 0.7636\n",
            "Epoch 3/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.1538 - accuracy: 0.9396 - val_loss: 0.3066 - val_accuracy: 0.8980\n",
            "Epoch 4/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.1095 - accuracy: 0.9579 - val_loss: 0.9221 - val_accuracy: 0.7604\n",
            "Epoch 5/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.0903 - accuracy: 0.9656 - val_loss: 0.3653 - val_accuracy: 0.8928\n",
            "Epoch 6/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.0719 - accuracy: 0.9721 - val_loss: 0.4104 - val_accuracy: 0.8768\n",
            "Epoch 7/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.0711 - accuracy: 0.9724 - val_loss: 0.3480 - val_accuracy: 0.8788\n",
            "Epoch 8/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.0517 - accuracy: 0.9806 - val_loss: 0.5497 - val_accuracy: 0.8684\n",
            "Epoch 9/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.0523 - accuracy: 0.9805 - val_loss: 0.6451 - val_accuracy: 0.8468\n",
            "Epoch 10/10\n",
            "704/704 [==============================] - 6s 9ms/step - loss: 0.0419 - accuracy: 0.9844 - val_loss: 0.5414 - val_accuracy: 0.8640\n",
            "782/782 [==============================] - 3s 4ms/step - loss: 0.6108 - accuracy: 0.8460\n",
            "loss = 0.6107993125915527\n",
            "acc = 0.8459600210189819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRMCdBG6XCVT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}