{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_addition_using_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJ1rebQqJ-p"
      },
      "source": [
        "# eq2seq 학습을 사용한 숫자 덧셈\n",
        "\n",
        "\n",
        "**작성자:** [임도형](http://github.com/dhrim)<br>\n",
        "**원 작성자:** [Smerity](https://twitter.com/Smerity)와 다른이들<br>\n",
        "**원 문서:** https://keras.io/examples/nlp/addition_rnn/<br>\n",
        "**작성일:** 2020/08/22<br>\n",
        "**최종 수정일:** 2020/08/22<br>\n",
        "**개요:** 숫자 덧셈 문자열의 입력으로 값 문자열을 구한다. 예 \"535+61\" -> \"596\"<br>\n",
        "**원 개요:** A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\".\n",
        "\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Qgsg_W497m"
      },
      "source": [
        "# 소개\n",
        "\n",
        "본 예는 2개의 숫자를 더한 값을 문자열로 출력하는 모델을 학습시킨다.\n",
        "\n",
        "**예:**\n",
        "- 입력 : \"535+61\"\n",
        "- 출력 : \"506\"\n",
        "\n",
        "다음과 같은 다양한 작업에서 보여지듯이 성능을 위해 입력은 선택적으로 연순이 될 수 있다. [Learning to Execute](http://arxiv.org/abs/1410.4615), \n",
        "[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
        "\n",
        "이론적으로 순차열 역순은 이런 문제에서 입력과 출력 간에 더 의존성을 더 짧게 한다.\n",
        "\n",
        "**결과**:<br>\n",
        "2자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 5K 학습 데이터 = 55 에폭후에 99% 학습/평가 정확도\n",
        "\n",
        "3자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 50K 학습 데이터 = 100 에폭후에 99% 학습/평가 정확도\n",
        "\n",
        "4자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 400K 학습 데이터 = 20 에폭후에 99% 학습/평가 정확도\n",
        "\n",
        "5자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 128K 학습 데이터 = 30 에폭후에 99% 학습/평가 정확도 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ux6UDaa493R"
      },
      "source": [
        "\n",
        "# 예제 설명\n",
        "\n",
        "# Seq2Seq 학습\n",
        "\n",
        "순차열(sequence)를 입력으로 순차열(sequence)를 출력하는 모델 학습.\n",
        "\n",
        "보통 인코더 + 디코더의 구조입니다. 인코더는 입력 문자열을 처리하여 문맥(context)를 출력하고 디코더는 이 문맥을 디코딩하여 목적하는 출력을 내도록 학습합니다.\n",
        "\n",
        "\n",
        "참고 : https://wikidocs.net/24996\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## 학습 내용\n",
        "\n",
        "다음과 같은 입력문자열에 대하여 출력문자열을 타겟으로 학습한다.\n",
        "\n",
        "```\n",
        "입력문자열 --->  출력문자열\n",
        "'7+81   '  --->  '25  '\n",
        "'583+2  '  --->  '585 '\n",
        "'58+12  '  --->  '70  '\n",
        "'5+85   '  --->  '90  '\n",
        "```\n",
        "\n",
        "결과적으로 입력된 문자열에 대한 덧셈을 수행하는 관계를 학습한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 역순 입력\n",
        "\n",
        "성능을 위해서 엽력 문자열을 역순으로 해서 사용한다.\n",
        "\n",
        "```\n",
        "입력문자열 --->  출력문자열\n",
        "'   18+7'  --->  '25  '\n",
        "'  2+385'  --->  '585 '\n",
        "'  21+85'  --->  '70  '\n",
        "'   58+5'  --->  '90  '\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "## 데이터\n",
        "\n",
        "### 입출력 데이터의 길이\n",
        "숫자의 최대 길이(DIGIT)가 3이면 2개의 숫자와 덧셈기호의 최대 길이는 7이다. 출력은 최대 4(3자리 숫자의 합은 최대 4자리)이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### one-hot 인코딩\n",
        "실제 학습에 사용되는 데이터는 각 문자열이 one-hot 인코딩된 값이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### 입력 데이터 모양\n",
        "입력 문자열의 길이는 7이고 각 문자는 12개 길이로 one-hot 인코딩된다.\n",
        "\n",
        "사용되는 문자는 0~9까지 숫자 10개와 덧셈기호와 스페이스 해서 총 12개이다. 입력 문자열의 길이는 7이고 1개의 문자는 길이 12의 one-hot 인코딩된 값이라서 1개 데이터의 모양은 (7,12)이다.\n",
        "\n",
        "입력 문자열이 ' 13+528'이라면 실제 학습에 사용되는 입력값은 다음과 같다.\n",
        "```\n",
        "[[1 0 0 0 0 0 0 0 0 0 0 0]    <--- ' '\n",
        " [0 0 0 1 0 0 0 0 0 0 0 0]    <--- '1'\n",
        " [0 0 0 0 0 1 0 0 0 0 0 0]    <--- '3'\n",
        " [0 1 0 0 0 0 0 0 0 0 0 0]    <--- '+'\n",
        " [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'    \n",
        " [0 0 0 0 1 0 0 0 0 0 0 0]    <--- '2'\n",
        " [0 0 0 0 0 0 0 0 0 0 1 0]]   <--- '8'\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### 출력 데이터 모양\n",
        "출력 문자열이 '856 '이라면 실제 학습에 사용되는 출력값은 다음과 같다.\n",
        "\n",
        "```        \n",
        "[[0 0 0 0 0 0 0 0 0 0 1 0]    <--- '8'\n",
        " [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'\n",
        " [0 0 0 0 0 0 0 0 1 0 0 0]    <--- '6'\n",
        " [1 0 0 0 0 0 0 0 0 0 0 0]]   <--- ' '     \n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "## 모델\n",
        "\n",
        "### 모델 입출력\n",
        "입력 모양은 (7,12)이고 출력 모양은 (4,12)이다.\n",
        "\n",
        "7은 입력문자열의 최대 길이, 4는 출력문자열의 최대 길이, 그리고 12는 입출력에 사용되는 문자의 갯수가 12라서 one-hot 인코딩된 값의 길이이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### Question-Answer 모델\n",
        "\n",
        "Question-Answer 구조의 모델이다. 질문 무자열이 입력되고 이를 encoder가 인코딩한다. 그리고 그 값을 다시  디코더가 decoding해서 답을 구한다.\n",
        "\n",
        "2개의 LSTM이 각각 인코더, 디코더로 사용된다.\n",
        "\n",
        "<br>\n",
        "\n",
        "다음 구조의 모델을 사용한다.\n",
        "```\n",
        "model = keras.Sequential()\n",
        "model.add(LSTM(..., input_shape=(MAXLEN, 12)) # encoder\n",
        "model.add(RepeatVector(DIGIT+1))\n",
        "model.add(LSTM(..., return_sequences=True)) # decoder\n",
        "model.add(Dense(12, activation=\"softmax\"))\n",
        "```\n",
        "2개의 LSTM이 있고 첫번째가 인코더이고, 두번째가 디코더이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "출력층 Dense의 노드 수가 12개 인것은 12개의 문자중에 1개를 선택하기 위한 것이다. 그래서 activation이 softmax이고.\n",
        "\n",
        "전체 모델의 출력은 one-hot 인코딩된 4개의 자리값이어야 한다. 이를 위해 4개의 타임 스텝이 필요하다. 이를 위해 디코더 LSTM앞에 RepeatedVector(4)가 있어서 인코딩된 128 차원의 값을 4번 반복한다.\n",
        "\n",
        "결과적으로 디코더 LSTM은 4개의 값을 출력하고 Dense는 각각 4개의 값에 대한 문자열을 softmax로 출력한다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou-ygxNfo-Ec"
      },
      "source": [
        "## 셋업\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5OvBIiWo-Ec"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "TRAINING_SIZE = 50000   # 학습 데이터의 갯수\n",
        "DIGITS = 3              # 덧셈에 사용될 숫자 자리수\n",
        "REVERSE = True          # 입력 문자열의 역순 여부. \n",
        "\n",
        "# 입력 문자열의 최대 길이.\n",
        "# 숫자 2개의 길이 에 '+' 길이 1을 더한 값이다.\n",
        "# 숫자 자리수가 3이면 예를 들어 '345+678'과 같이 최대 길이는 7이다.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dQt0G10o-Eg"
      },
      "source": [
        "## Generate the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COsdmY5vo-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b67e60-fd02-44d8-a1c9-210b82f3cd7d"
      },
      "source": [
        "# 사용될 문자들. 0~9까지 숫자 10개와 덧셈 '+'와 스페이스 ' '\n",
        "chars = \"0123456789+ \"\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "\n",
        "    # 임의의 숫자를 만드는 함수\n",
        "    # step1. 최대 자리수를 임의로 선택하고. 최소 1, 최대 3(DIGIT). 선택된 것이 3이라면\n",
        "    # step2. 각 자리의 숫자를 선택하고. '1', '3', '5'\n",
        "    # step3. 각 자리의 숫자를 붙여서 문자열 만들고. '135'\n",
        "    # step4. 만든 문자열을 int로 변환. '135' -> 135\n",
        "    f = lambda: int( # step4\n",
        "        \"\".join( # step3\n",
        "            np.random.choice(list(\"0123456789\")) # step2\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))  # step1\n",
        "        )\n",
        "    )\n",
        "    # 2개의 숫자를 선택했다.\n",
        "    a, b = f(), f()\n",
        "    # a = 123\n",
        "    # b = 45\n",
        "\n",
        "    # 이미 만들었던 문제면 다시 만들자.\n",
        "    # x+y와 y+x의 중복도 체크하기 위해 소팅해서 체크한다.\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "\n",
        "    # 문제 문자열을 만들자\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    # q = '123+45'\n",
        "\n",
        "    # 문자열 길이가 MAXLEN이 되도록 스페이스를 뒤에 붙인다.\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    # query = '123+45 '\n",
        "\n",
        "    # 정답 문자열을 구한다.\n",
        "    ans = str(a + b)\n",
        "    # ans = '168'\n",
        "\n",
        "    # 정답은 최대 DIGITS + 1 길이이다. 최대 길이가 되도록 스페이스를 앞에 붙인다.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    # ans = ' 168'\n",
        "\n",
        "    # 역순으로 처리한다면\n",
        "    # 문제 문자열을 역순으로 한다.\n",
        "    if REVERSE:\n",
        "        # '123+45  '가 '  54+321'가 된다.\n",
        "        query = query[::-1]\n",
        "        # query = ' 54+321'\n",
        "\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "\n",
        "print(\"Total questions:\", len(questions))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7eFX6jIyrnx"
      },
      "source": [
        "# 데이터를 벡터화 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO8skDb7p3f"
      },
      "source": [
        "인코딩 디코딩을 위한 유틸 클래스 CharacterTable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqn4LUtHyi46"
      },
      "source": [
        "class CharacterTable:\n",
        "    \"\"\" 특정 문자열 집합을 가지고:\n",
        "    + one-hot 인코딩\n",
        "    + one-hot된 혹은 숫자로 인코딩된 값을 원 무자열로 디코딩\n",
        "    + softmax 같은 확율 벡터를 원 문자열로 디코딩\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"문자 테이블을 초기화\n",
        "        # Arguments\n",
        "            chars: 입력에 사용되는 문자들\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        # self.char_indices = { ' ': 0, \n",
        "        #                       '+': 1,\n",
        "        #                       '0': 2, \n",
        "        #                       '1': 3, \n",
        "        #                       ...\n",
        "        #                       '9': 11}\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "        # self.indices_char = { 0: ' ', \n",
        "        #                       1: '+', \n",
        "        #                       2: '0', \n",
        "        #                       3: '1', \n",
        "        #                       ...\n",
        "        #                       11: '9'}\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"전달된 문자열 C를 One-hot 인코딩 한다.\n",
        "        # Arguments\n",
        "            C: 인코딩할 문자열. 예 ' 123+45'\n",
        "            num_rows: 반환될 행 수. 입력 길이와 관계없이 반환되는 행 수를 동일하기 위해 사용된다.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C): # 각 자리 문자마다\n",
        "            x[i, self.char_indices[c]] = 1 # 각 줄의 index만 1로 만든다. one-hot encoding한다.\n",
        "\n",
        "        # 반환되는 x는 num_rows의 행이고, 각 행은 각 문자에 대한 one-hot 인코딩된 값이다.\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"전달된 벡터값 혹은 2D 배열을 해당 문자열로 디코딩한다.\n",
        "        # Arguments\n",
        "            x: 벡터 혹은 one-hot 인코딩된 2D 배열 혹은 softmax된 확률 2D 배열 \n",
        "               혹은 calc_argmax=False일 때는 문자 인덱스 벡터\n",
        "            calc_argmax: 최대 값의 index 값을 찾을 지 여부. default는 True.\n",
        "        \"\"\"\n",
        "\n",
        "        # one-hot 인코딩 혹은 softmax 값이면 최대 index를 구한다.\n",
        "        if calc_argmax:\n",
        "            # x = [ [0 0 0 0 1 0 0 0 0 0 0 0]\n",
        "            #       [0 0 0 0 0 1 0 0 0 0 0 0]\n",
        "            #       [0 0 0 0 0 0 1 0 0 0 0 0]]\n",
        "            x = x.argmax(axis=-1)\n",
        "            # x = [ 4 5 6 ]\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVYzei1I7XZb"
      },
      "source": [
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_fJZA7b7Y-e"
      },
      "source": [
        "다음은 인코딩, 디코딩 예이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwm40AA-r4tt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6560df74-c3be-4623-eb01-b0c426d01a21"
      },
      "source": [
        "# 원 문자열\n",
        "org_str = ' 123+45'\n",
        "print(f\"org_str='{org_str}'\")\n",
        "\n",
        "# 인코딩된 ㄱ밧\n",
        "encoded = ctable.encode(org_str, MAXLEN)\n",
        "# [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- ' '\n",
        "#  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- '1'\n",
        "#  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]      <--- '2'\n",
        "#  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]      <--- '3'\n",
        "#  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- '+'\n",
        "#  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]      <--- '4'\n",
        "#  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]     <--- '5'\n",
        "print(f\"encoded=\\n{encoded}\")\n",
        "print(f\"encoded.shape={encoded.shape}\")\n",
        "\n",
        "# 다시 디코딩해서 원복한 값\n",
        "decoded = ctable.decode(encoded)\n",
        "print(f\"decoded='{decoded}'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "org_str=' 123+45'\n",
            "encoded=\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "encoded.shape=(7, 12)\n",
            "decoded=' 123+45'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1QHj3MDo-Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecd293bc-8a1f-46a9-ee8b-518f7858d9fd"
      },
      "source": [
        "print(\"Vectorization...\")\n",
        "\n",
        "# DIGITS = 3\n",
        "# MAXLEN = DIGITS + 1 + DIGITS\n",
        "# chars = '0123456789+'\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "# x 데이터 한개의 모양은 (7, 12)  # 7 글자, 12개 글자 종류(0~9, '+', ' ')\n",
        "# y 데이터 한개의 모양은 (3, 12)  # 3 글자, 12개 글자 종류(0~9, '+', ' ')\n",
        "\n",
        "# x의 뒷쪽은 대부분 큰 자리수의 문제이다.\n",
        "# 이를 커버하기 위해 섞는다.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# 명시적으로 10%는 학습에 사용되지 않는 validation 데이터로 딸 둔다.\n",
        "# 역자주 : 보통 데이터는 train/validation/test로 분리합니다. \n",
        "# 학습 시에 사용하지 않는 것은 test 데이터인데 저자가 혼동한 듯 합니다.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMq2Om3w8qNy"
      },
      "source": [
        "준비된 데이터는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41UcKyn-q5Ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34258cc0-eeb6-423d-9d2d-d5d04cacab65"
      },
      "source": [
        "print(\"x_train[0]\\n\", x_train[0].astype(np.int))\n",
        "print()\n",
        "print(\"y_train[0]\\n\", y_train[0].astype(np.int))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train[0]\n",
            " [[1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0]]\n",
            "\n",
            "y_train[0]\n",
            " [[0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-gM3Nq84XK"
      },
      "source": [
        "준비된 데이터는 인코딩된 값들입니다. 디코딩에서 본 원 문자열은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw_RFfJvrGFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d01416c0-7a28-4545-94a2-84e1411c135e"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"'\"+ctable.decode(x_train[i])+\"' -> '\"+ctable.decode(y_train[i])+\"'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "' 55+916' -> '674 '\n",
            "' 491+77' -> '271 '\n",
            "' 782+26' -> '349 '\n",
            "'964+035' -> '999 '\n",
            "' 68+152' -> '337 '\n",
            "'  2+836' -> '640 '\n",
            "'  1+249' -> '943 '\n",
            "' 726+82' -> '655 '\n",
            "' 08+982' -> '369 '\n",
            "' 81+104' -> '419 '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MkDWbri9EAd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2liXf2Yo-En"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP8Vw2aNtzPn"
      },
      "source": [
        "# 모델 정의\n",
        "\n",
        "LSTM을 사용. 모델은 다음과 같다.\n",
        "\n",
        "```\n",
        "model.add(layers.LSTM(..., input_shape=(7, 12)) # 7:입력 최대 길이, 12:문자 종류 수\n",
        "model.add(RepeatVector(4)) # 4:숫자 최대길이 3+1. 4자리가 되도록 값을 반복한다.\n",
        "model.add(LSTM(..., return_sequences=True)\n",
        "model.add(Dense(12, activation=\"softmax\")) # 12:문자 종류 수\n",
        "```\n",
        "\n",
        "각 층의 출력은 다음과 같다.\n",
        "```\n",
        "LSTM            (None, 128)\n",
        "RepeatVector    (None, 4, 128)    # [1,2] -> [ [1,2], [1,2], [1,2], [1,2] ]\n",
        "LSTM            (None, 4, 128)\n",
        "Dense           (None, 4, 12)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUr-sqkoo-En",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cf40c0-bc97-4b41-d550-b27589bc8bf8"
      },
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# 입력 순차열을 LSTM을 사용하여 128길이의 출력으로 인코딩한다.\n",
        "# 주의 : 입력 길이가 변하는 상황이면 input_shape=(None, num_feature)로 한다.\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "\n",
        "# 디코더 입력으로 인코딩된 값을 각 시간 스텝(출력 자리) 수 만큼 반복해 준다.\n",
        "# 출력 최대 길이인 DIGIT+1만큼 반복한다. \n",
        "# DIGIT가 3일때 최대 출력값은 999+999=1998이다.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "\n",
        "# 디코더 RNN은 한 층 혹은 여러층일 수 있다.\n",
        "for _ in range(num_layers):\n",
        "    # return_sequences을 True로 놓아 마지막 스텝뿐 아니라 전체 스텝의 출력값을 \n",
        "    # (num_samples, timesteps, output_dimc) 모양으로 출력한다.\n",
        "    # 이것은 다음 층에서 사용되는 TimeDistributed가 입력의 첫 차원이 시간 스텝이어야 하기 때문이다.\n",
        "    #\n",
        "    # 역자주 : TensorFlow 2.0이전에는 model.add(TimeDistributed(Dense(..)))의 형태로 쓰여야 했다.\n",
        "    # 주석에서 TimeDistributed에 관련된 것은 아마도 코드를 업데이트 하면서 주석은 업데이트 하지 않은 것 같다.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# 각 스텝의 출력에 대한 문자열을 선택하도록 한다.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5L-7r5Yo-Er"
      },
      "source": [
        "## 모델 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Zav9Fro-Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a0a5695-adec-4524-fbc4-13ec545aea2d"
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "# 학습을 실행하고, 각 학습 중에 validation 데이터에 대한 결과를 보여준다.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "\n",
        "    # 10개 랜덤 샘플을 골라 결과를 보여준다.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        # rowx = [[[1 0 0 0 0 0 0 0 0 0 0 0]    <--- ' '\n",
        "        #          [0 0 0 1 0 0 0 0 0 0 0 0]    <--- '1'\n",
        "        #          [0 0 0 0 0 1 0 0 0 0 0 0]    <--- '3'\n",
        "        #          [0 1 0 0 0 0 0 0 0 0 0 0]    <--- '+'\n",
        "        #          [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'    \n",
        "        #          [0 0 0 0 1 0 0 0 0 0 0 0]    <--- '2'\n",
        "        #          [0 0 0 0 0 0 0 0 0 0 1 0]]]  <--- '8'\n",
        "        #                                            ' 13+528'\n",
        "        #\n",
        "        # rowy = [[[0 0 0 0 0 0 0 0 0 0 1 0]    <--- '8'\n",
        "        #          [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'\n",
        "        #          [0 0 0 0 0 0 0 0 1 0 0 0]    <--- '6'\n",
        "        #          [1 0 0 0 0 0 0 0 0 0 0 0]]]  <--- ' '     \n",
        "        #                                            '856 '   \n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        # preds = [[10  7  8  0]]\n",
        "        # 요 preds 값은 CharacterTable에 정의된 indices_char의 키 값에 해당한다.\n",
        "        #   {0: ' ',   # <------\n",
        "        #    1: '+', \n",
        "        #    ...\n",
        "        #    7: '5'    # <------\n",
        "        #    8: '6'    # <------\n",
        "        #    9: '7'\n",
        "        #    10: '8'   # <------\n",
        "        #    11: '9'}        \n",
        "\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        #  q       = ' 13+528'  <--- '825+31 '가 reverse된 것.\n",
        "        #  correct = '856 '\n",
        "        #  guess   = '856 '        \n",
        "        \n",
        "        # REVERSE = True\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        # Q 825+31\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        # T 856  \n",
        "        if correct == guess:\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)\n",
        "        # ☑ 856             \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1\n",
            "1407/1407 [==============================] - 13s 5ms/step - loss: 1.7457 - accuracy: 0.3597 - val_loss: 1.5397 - val_accuracy: 0.4214\n",
            "Q 2+330   T 332  ☒ 334 \n",
            "Q 5+564   T 569  ☒ 556 \n",
            "Q 34+709  T 743  ☒ 419 \n",
            "Q 898+532 T 1430 ☒ 1399\n",
            "Q 35+62   T 97   ☒ 10  \n",
            "Q 957+4   T 961  ☒ 966 \n",
            "Q 918+74  T 992  ☒ 901 \n",
            "Q 320+35  T 355  ☒ 346 \n",
            "Q 78+163  T 241  ☒ 119 \n",
            "Q 534+95  T 629  ☒ 599 \n",
            "\n",
            "Iteration 2\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.3320 - accuracy: 0.4992 - val_loss: 1.1828 - val_accuracy: 0.5497\n",
            "Q 577+99  T 676  ☒ 631 \n",
            "Q 889+852 T 1741 ☒ 1732\n",
            "Q 639+56  T 695  ☒ 691 \n",
            "Q 550+434 T 984  ☒ 902 \n",
            "Q 765+24  T 789  ☒ 782 \n",
            "Q 127+24  T 151  ☒ 132 \n",
            "Q 71+701  T 772  ☒ 771 \n",
            "Q 74+89   T 163  ☒ 142 \n",
            "Q 73+303  T 376  ☒ 338 \n",
            "Q 85+444  T 529  ☒ 513 \n",
            "\n",
            "Iteration 3\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0550 - accuracy: 0.6069 - val_loss: 0.9621 - val_accuracy: 0.6463\n",
            "Q 769+83  T 852  ☒ 856 \n",
            "Q 29+133  T 162  ☒ 166 \n",
            "Q 215+814 T 1029 ☒ 1066\n",
            "Q 869+59  T 928  ☒ 920 \n",
            "Q 709+736 T 1445 ☒ 1435\n",
            "Q 90+90   T 180  ☑ 180 \n",
            "Q 52+139  T 191  ☒ 185 \n",
            "Q 676+334 T 1010 ☒ 1002\n",
            "Q 735+1   T 736  ☒ 732 \n",
            "Q 72+96   T 168  ☒ 160 \n",
            "\n",
            "Iteration 4\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.9033 - accuracy: 0.6647 - val_loss: 0.8422 - val_accuracy: 0.6868\n",
            "Q 523+986 T 1509 ☒ 1503\n",
            "Q 981+967 T 1948 ☒ 1946\n",
            "Q 2+215   T 217  ☒ 218 \n",
            "Q 263+178 T 441  ☒ 430 \n",
            "Q 84+555  T 639  ☒ 640 \n",
            "Q 86+357  T 443  ☑ 443 \n",
            "Q 8+511   T 519  ☒ 529 \n",
            "Q 48+393  T 441  ☒ 433 \n",
            "Q 317+30  T 347  ☒ 345 \n",
            "Q 854+3   T 857  ☒ 858 \n",
            "\n",
            "Iteration 5\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.7977 - accuracy: 0.7058 - val_loss: 0.7704 - val_accuracy: 0.7085\n",
            "Q 115+116 T 231  ☒ 238 \n",
            "Q 9+320   T 329  ☒ 328 \n",
            "Q 975+28  T 1003 ☒ 1002\n",
            "Q 471+114 T 585  ☒ 596 \n",
            "Q 141+96  T 237  ☒ 233 \n",
            "Q 36+335  T 371  ☒ 376 \n",
            "Q 875+70  T 945  ☒ 943 \n",
            "Q 490+495 T 985  ☒ 963 \n",
            "Q 699+42  T 741  ☒ 743 \n",
            "Q 409+8   T 417  ☒ 413 \n",
            "\n",
            "Iteration 6\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.6889 - accuracy: 0.7463 - val_loss: 0.6195 - val_accuracy: 0.7724\n",
            "Q 314+9   T 323  ☒ 324 \n",
            "Q 28+329  T 357  ☒ 354 \n",
            "Q 230+694 T 924  ☒ 925 \n",
            "Q 814+927 T 1741 ☒ 1740\n",
            "Q 868+639 T 1507 ☒ 1500\n",
            "Q 334+31  T 365  ☑ 365 \n",
            "Q 97+323  T 420  ☒ 429 \n",
            "Q 263+7   T 270  ☑ 270 \n",
            "Q 498+99  T 597  ☒ 585 \n",
            "Q 697+194 T 891  ☒ 777 \n",
            "\n",
            "Iteration 7\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.4480 - accuracy: 0.8445 - val_loss: 0.3451 - val_accuracy: 0.8858\n",
            "Q 312+8   T 320  ☑ 320 \n",
            "Q 72+90   T 162  ☒ 163 \n",
            "Q 911+299 T 1210 ☑ 1210\n",
            "Q 33+534  T 567  ☑ 567 \n",
            "Q 68+415  T 483  ☑ 483 \n",
            "Q 396+2   T 398  ☑ 398 \n",
            "Q 7+116   T 123  ☑ 123 \n",
            "Q 91+396  T 487  ☒ 488 \n",
            "Q 962+96  T 1058 ☑ 1058\n",
            "Q 9+392   T 401  ☑ 401 \n",
            "\n",
            "Iteration 8\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.2593 - accuracy: 0.9245 - val_loss: 0.2104 - val_accuracy: 0.9409\n",
            "Q 512+975 T 1487 ☒ 1486\n",
            "Q 63+617  T 680  ☑ 680 \n",
            "Q 39+783  T 822  ☑ 822 \n",
            "Q 6+589   T 595  ☒ 596 \n",
            "Q 659+140 T 799  ☒ 890 \n",
            "Q 792+4   T 796  ☑ 796 \n",
            "Q 620+81  T 701  ☑ 701 \n",
            "Q 650+634 T 1284 ☑ 1284\n",
            "Q 362+87  T 449  ☑ 449 \n",
            "Q 30+832  T 862  ☑ 862 \n",
            "\n",
            "Iteration 9\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1718 - accuracy: 0.9547 - val_loss: 0.1309 - val_accuracy: 0.9670\n",
            "Q 57+27   T 84   ☑ 84  \n",
            "Q 163+267 T 430  ☑ 430 \n",
            "Q 4+932   T 936  ☑ 936 \n",
            "Q 127+382 T 509  ☑ 509 \n",
            "Q 359+917 T 1276 ☑ 1276\n",
            "Q 958+84  T 1042 ☒ 1041\n",
            "Q 607+83  T 690  ☑ 690 \n",
            "Q 850+885 T 1735 ☑ 1735\n",
            "Q 330+550 T 880  ☒ 881 \n",
            "Q 918+74  T 992  ☑ 992 \n",
            "\n",
            "Iteration 10\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.1142 - accuracy: 0.9715 - val_loss: 0.1131 - val_accuracy: 0.9672\n",
            "Q 523+601 T 1124 ☒ 1125\n",
            "Q 750+338 T 1088 ☑ 1088\n",
            "Q 928+8   T 936  ☑ 936 \n",
            "Q 45+613  T 658  ☑ 658 \n",
            "Q 836+48  T 884  ☑ 884 \n",
            "Q 506+423 T 929  ☑ 929 \n",
            "Q 719+341 T 1060 ☒ 1050\n",
            "Q 43+40   T 83   ☑ 83  \n",
            "Q 683+225 T 908  ☑ 908 \n",
            "Q 50+195  T 245  ☑ 245 \n",
            "\n",
            "Iteration 11\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0920 - accuracy: 0.9761 - val_loss: 0.0664 - val_accuracy: 0.9857\n",
            "Q 295+827 T 1122 ☑ 1122\n",
            "Q 29+84   T 113  ☑ 113 \n",
            "Q 208+4   T 212  ☑ 212 \n",
            "Q 73+598  T 671  ☑ 671 \n",
            "Q 368+21  T 389  ☑ 389 \n",
            "Q 126+697 T 823  ☑ 823 \n",
            "Q 62+890  T 952  ☑ 952 \n",
            "Q 134+3   T 137  ☑ 137 \n",
            "Q 60+2    T 62   ☑ 62  \n",
            "Q 707+80  T 787  ☑ 787 \n",
            "\n",
            "Iteration 12\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0717 - accuracy: 0.9818 - val_loss: 0.0647 - val_accuracy: 0.9827\n",
            "Q 7+62    T 69   ☑ 69  \n",
            "Q 901+39  T 940  ☑ 940 \n",
            "Q 293+70  T 363  ☑ 363 \n",
            "Q 354+332 T 686  ☑ 686 \n",
            "Q 67+191  T 258  ☑ 258 \n",
            "Q 848+852 T 1700 ☑ 1700\n",
            "Q 7+207   T 214  ☑ 214 \n",
            "Q 909+741 T 1650 ☑ 1650\n",
            "Q 615+3   T 618  ☑ 618 \n",
            "Q 244+2   T 246  ☑ 246 \n",
            "\n",
            "Iteration 13\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0586 - accuracy: 0.9856 - val_loss: 0.1327 - val_accuracy: 0.9556\n",
            "Q 808+196 T 1004 ☑ 1004\n",
            "Q 587+13  T 600  ☑ 600 \n",
            "Q 600+8   T 608  ☑ 608 \n",
            "Q 162+405 T 567  ☒ 667 \n",
            "Q 683+225 T 908  ☒ 9088\n",
            "Q 655+286 T 941  ☑ 941 \n",
            "Q 11+717  T 728  ☑ 728 \n",
            "Q 1+787   T 788  ☑ 788 \n",
            "Q 95+885  T 980  ☑ 980 \n",
            "Q 85+549  T 634  ☑ 634 \n",
            "\n",
            "Iteration 14\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0403 - accuracy: 0.9905 - val_loss: 0.0341 - val_accuracy: 0.9919\n",
            "Q 592+6   T 598  ☑ 598 \n",
            "Q 70+909  T 979  ☑ 979 \n",
            "Q 132+833 T 965  ☑ 965 \n",
            "Q 69+966  T 1035 ☑ 1035\n",
            "Q 22+513  T 535  ☑ 535 \n",
            "Q 928+66  T 994  ☑ 994 \n",
            "Q 64+283  T 347  ☑ 347 \n",
            "Q 39+507  T 546  ☑ 546 \n",
            "Q 357+4   T 361  ☑ 361 \n",
            "Q 87+374  T 461  ☑ 461 \n",
            "\n",
            "Iteration 15\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0477 - accuracy: 0.9872 - val_loss: 0.0192 - val_accuracy: 0.9969\n",
            "Q 85+713  T 798  ☑ 798 \n",
            "Q 708+87  T 795  ☑ 795 \n",
            "Q 439+57  T 496  ☑ 496 \n",
            "Q 259+56  T 315  ☑ 315 \n",
            "Q 27+46   T 73   ☑ 73  \n",
            "Q 6+906   T 912  ☑ 912 \n",
            "Q 8+29    T 37   ☑ 37  \n",
            "Q 259+59  T 318  ☑ 318 \n",
            "Q 498+7   T 505  ☑ 505 \n",
            "Q 458+996 T 1454 ☑ 1454\n",
            "\n",
            "Iteration 16\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0466 - accuracy: 0.9866 - val_loss: 0.0262 - val_accuracy: 0.9944\n",
            "Q 860+98  T 958  ☑ 958 \n",
            "Q 2+569   T 571  ☑ 571 \n",
            "Q 14+196  T 210  ☑ 210 \n",
            "Q 99+386  T 485  ☑ 485 \n",
            "Q 48+797  T 845  ☑ 845 \n",
            "Q 817+37  T 854  ☑ 854 \n",
            "Q 80+452  T 532  ☑ 532 \n",
            "Q 328+14  T 342  ☑ 342 \n",
            "Q 376+561 T 937  ☑ 937 \n",
            "Q 549+612 T 1161 ☑ 1161\n",
            "\n",
            "Iteration 17\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0356 - accuracy: 0.9906 - val_loss: 0.0218 - val_accuracy: 0.9961\n",
            "Q 74+911  T 985  ☑ 985 \n",
            "Q 978+562 T 1540 ☑ 1540\n",
            "Q 32+48   T 80   ☑ 80  \n",
            "Q 84+173  T 257  ☑ 257 \n",
            "Q 1+909   T 910  ☑ 910 \n",
            "Q 736+62  T 798  ☑ 798 \n",
            "Q 247+63  T 310  ☑ 310 \n",
            "Q 444+73  T 517  ☑ 517 \n",
            "Q 71+706  T 777  ☑ 777 \n",
            "Q 736+62  T 798  ☑ 798 \n",
            "\n",
            "Iteration 18\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0329 - accuracy: 0.9908 - val_loss: 0.0183 - val_accuracy: 0.9962\n",
            "Q 15+460  T 475  ☑ 475 \n",
            "Q 444+73  T 517  ☑ 517 \n",
            "Q 85+849  T 934  ☑ 934 \n",
            "Q 19+35   T 54   ☑ 54  \n",
            "Q 785+434 T 1219 ☑ 1219\n",
            "Q 34+96   T 130  ☑ 130 \n",
            "Q 284+39  T 323  ☑ 323 \n",
            "Q 91+502  T 593  ☑ 593 \n",
            "Q 918+874 T 1792 ☑ 1792\n",
            "Q 90+658  T 748  ☑ 748 \n",
            "\n",
            "Iteration 19\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0278 - accuracy: 0.9926 - val_loss: 0.0233 - val_accuracy: 0.9939\n",
            "Q 74+436  T 510  ☑ 510 \n",
            "Q 154+430 T 584  ☑ 584 \n",
            "Q 446+67  T 513  ☑ 513 \n",
            "Q 75+7    T 82   ☑ 82  \n",
            "Q 29+616  T 645  ☑ 645 \n",
            "Q 89+455  T 544  ☑ 544 \n",
            "Q 3+394   T 397  ☑ 397 \n",
            "Q 57+43   T 100  ☑ 100 \n",
            "Q 152+69  T 221  ☑ 221 \n",
            "Q 93+592  T 685  ☑ 685 \n",
            "\n",
            "Iteration 20\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0308 - accuracy: 0.9912 - val_loss: 0.0204 - val_accuracy: 0.9950\n",
            "Q 9+129   T 138  ☑ 138 \n",
            "Q 196+40  T 236  ☑ 236 \n",
            "Q 505+91  T 596  ☑ 596 \n",
            "Q 775+90  T 865  ☑ 865 \n",
            "Q 91+409  T 500  ☒ 490 \n",
            "Q 375+0   T 375  ☑ 375 \n",
            "Q 89+427  T 516  ☑ 516 \n",
            "Q 681+44  T 725  ☑ 725 \n",
            "Q 4+540   T 544  ☑ 544 \n",
            "Q 15+460  T 475  ☑ 475 \n",
            "\n",
            "Iteration 21\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0318 - accuracy: 0.9913 - val_loss: 0.0096 - val_accuracy: 0.9982\n",
            "Q 194+732 T 926  ☑ 926 \n",
            "Q 550+645 T 1195 ☑ 1195\n",
            "Q 456+81  T 537  ☑ 537 \n",
            "Q 727+98  T 825  ☑ 825 \n",
            "Q 26+154  T 180  ☑ 180 \n",
            "Q 89+948  T 1037 ☑ 1037\n",
            "Q 7+906   T 913  ☑ 913 \n",
            "Q 156+760 T 916  ☑ 916 \n",
            "Q 436+57  T 493  ☑ 493 \n",
            "Q 582+336 T 918  ☑ 918 \n",
            "\n",
            "Iteration 22\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0283 - accuracy: 0.9920 - val_loss: 0.0122 - val_accuracy: 0.9973\n",
            "Q 326+25  T 351  ☑ 351 \n",
            "Q 187+858 T 1045 ☑ 1045\n",
            "Q 557+176 T 733  ☑ 733 \n",
            "Q 712+27  T 739  ☑ 739 \n",
            "Q 340+7   T 347  ☑ 347 \n",
            "Q 692+628 T 1320 ☑ 1320\n",
            "Q 20+37   T 57   ☑ 57  \n",
            "Q 410+32  T 442  ☑ 442 \n",
            "Q 370+909 T 1279 ☑ 1279\n",
            "Q 170+116 T 286  ☑ 286 \n",
            "\n",
            "Iteration 23\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0279 - accuracy: 0.9920 - val_loss: 0.0307 - val_accuracy: 0.9905\n",
            "Q 283+87  T 370  ☑ 370 \n",
            "Q 59+897  T 956  ☑ 956 \n",
            "Q 571+12  T 583  ☑ 583 \n",
            "Q 50+611  T 661  ☒ 651 \n",
            "Q 842+549 T 1391 ☑ 1391\n",
            "Q 957+410 T 1367 ☑ 1367\n",
            "Q 4+703   T 707  ☑ 707 \n",
            "Q 689+346 T 1035 ☑ 1035\n",
            "Q 88+27   T 115  ☑ 115 \n",
            "Q 639+872 T 1511 ☑ 1511\n",
            "\n",
            "Iteration 24\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.0957 - val_accuracy: 0.9694\n",
            "Q 45+603  T 648  ☑ 648 \n",
            "Q 89+350  T 439  ☑ 439 \n",
            "Q 18+951  T 969  ☒ 979 \n",
            "Q 99+909  T 1008 ☑ 1008\n",
            "Q 549+26  T 575  ☑ 575 \n",
            "Q 488+77  T 565  ☒ 564 \n",
            "Q 155+8   T 163  ☑ 163 \n",
            "Q 839+215 T 1054 ☑ 1054\n",
            "Q 597+89  T 686  ☑ 686 \n",
            "Q 2+892   T 894  ☑ 894 \n",
            "\n",
            "Iteration 25\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0229 - accuracy: 0.9937 - val_loss: 0.0114 - val_accuracy: 0.9970\n",
            "Q 31+814  T 845  ☑ 845 \n",
            "Q 875+1   T 876  ☑ 876 \n",
            "Q 967+9   T 976  ☑ 976 \n",
            "Q 621+771 T 1392 ☑ 1392\n",
            "Q 459+467 T 926  ☑ 926 \n",
            "Q 721+80  T 801  ☑ 801 \n",
            "Q 748+77  T 825  ☑ 825 \n",
            "Q 868+9   T 877  ☑ 877 \n",
            "Q 250+55  T 305  ☑ 305 \n",
            "Q 67+805  T 872  ☑ 872 \n",
            "\n",
            "Iteration 26\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0257 - accuracy: 0.9925 - val_loss: 0.0064 - val_accuracy: 0.9989\n",
            "Q 87+523  T 610  ☑ 610 \n",
            "Q 52+15   T 67   ☑ 67  \n",
            "Q 974+35  T 1009 ☑ 1009\n",
            "Q 89+455  T 544  ☑ 544 \n",
            "Q 30+70   T 100  ☑ 100 \n",
            "Q 809+523 T 1332 ☑ 1332\n",
            "Q 98+339  T 437  ☑ 437 \n",
            "Q 32+364  T 396  ☑ 396 \n",
            "Q 46+685  T 731  ☑ 731 \n",
            "Q 685+91  T 776  ☑ 776 \n",
            "\n",
            "Iteration 27\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0277 - accuracy: 0.9923 - val_loss: 0.0324 - val_accuracy: 0.9897\n",
            "Q 0+968   T 968  ☑ 968 \n",
            "Q 622+78  T 700  ☑ 700 \n",
            "Q 43+970  T 1013 ☑ 1013\n",
            "Q 158+325 T 483  ☑ 483 \n",
            "Q 685+37  T 722  ☑ 722 \n",
            "Q 793+820 T 1613 ☑ 1613\n",
            "Q 347+2   T 349  ☑ 349 \n",
            "Q 472+380 T 852  ☑ 852 \n",
            "Q 254+154 T 408  ☑ 408 \n",
            "Q 520+18  T 538  ☑ 538 \n",
            "\n",
            "Iteration 28\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0065 - accuracy: 0.9990 - val_loss: 0.0056 - val_accuracy: 0.9990\n",
            "Q 971+835 T 1806 ☑ 1806\n",
            "Q 24+48   T 72   ☑ 72  \n",
            "Q 736+62  T 798  ☑ 798 \n",
            "Q 593+631 T 1224 ☑ 1224\n",
            "Q 578+1   T 579  ☑ 579 \n",
            "Q 46+901  T 947  ☑ 947 \n",
            "Q 158+5   T 163  ☑ 163 \n",
            "Q 122+854 T 976  ☑ 976 \n",
            "Q 487+739 T 1226 ☑ 1226\n",
            "Q 164+5   T 169  ☑ 169 \n",
            "\n",
            "Iteration 29\n",
            "1407/1407 [==============================] - 7s 5ms/step - loss: 0.0260 - accuracy: 0.9926 - val_loss: 0.0176 - val_accuracy: 0.9944\n",
            "Q 19+455  T 474  ☑ 474 \n",
            "Q 76+943  T 1019 ☑ 1019\n",
            "Q 724+577 T 1301 ☑ 1301\n",
            "Q 200+494 T 694  ☑ 694 \n",
            "Q 602+42  T 644  ☑ 644 \n",
            "Q 18+951  T 969  ☑ 969 \n",
            "Q 869+59  T 928  ☑ 928 \n",
            "Q 267+216 T 483  ☑ 483 \n",
            "Q 314+53  T 367  ☑ 367 \n",
            "Q 23+431  T 454  ☑ 454 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3AO_JLlo-Eu"
      },
      "source": [
        "대략 30 epoch 뒤에 99+% 정도의 validation 정확도를 보인다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx4wwMdOBGCg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}