{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq2seq_addition_using_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOJ1rebQqJ-p"
      },
      "source": [
        "# eq2seq 학습을 사용한 숫자 덧셈\n",
        "\n",
        "\n",
        "**작성자:** [임도형](http://github.com/dhrim)<br>\n",
        "**원 작성자:** [Smerity](https://twitter.com/Smerity)와 다른이들<br>\n",
        "**원 문서:** https://keras.io/examples/nlp/addition_rnn/<br>\n",
        "**작성일:** 2020/08/22<br>\n",
        "**최종 수정일:** 2020/08/22<br>\n",
        "**개요:** 숫자 덧셈 문자열의 입력으로 값 문자열을 구한다. 예 \"535+61\" -> \"596\"<br>\n",
        "**원 개요:** A model that learns to add strings of numbers, e.g. \"535+61\" -> \"596\".\n",
        "\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Qgsg_W497m"
      },
      "source": [
        "# 소개\n",
        "\n",
        "본 예는 2개의 숫자를 더한 값을 문자열로 출력하는 모델을 학습시킨다.\n",
        "\n",
        "**예:**\n",
        "- 입력 : \"535+61\"\n",
        "- 출력 : \"506\"\n",
        "\n",
        "다음과 같은 다양한 작업에서 보여지듯이 성능을 위해 입력은 선택적으로 연순이 될 수 있다. [Learning to Execute](http://arxiv.org/abs/1410.4615), \n",
        "[Sequence to Sequence Learning with Neural Networks](http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)\n",
        "\n",
        "이론적으로 순차열 역순은 이런 문제에서 입력과 출력 간에 더 의존성을 더 짧게 한다.\n",
        "\n",
        "**결과**:<br>\n",
        "2자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 5K 학습 데이터 = 55 에폭후에 99% 학습/평가 정확도\n",
        "\n",
        "3자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 50K 학습 데이터 = 100 에폭후에 99% 학습/평가 정확도\n",
        "\n",
        "4자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 400K 학습 데이터 = 20 에폭후에 99% 학습/평가 정확도\n",
        "\n",
        "5자리(역순):<br>\n",
        "- 1 LSTM층(128 유닛), 128K 학습 데이터 = 30 에폭후에 99% 학습/평가 정확도 \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ux6UDaa493R"
      },
      "source": [
        "\n",
        "# 예제 설명\n",
        "\n",
        "# Seq2Seq 학습\n",
        "\n",
        "순차열(sequence)를 입력으로 순차열(sequence)를 출력하는 모델 학습.\n",
        "\n",
        "보통 인코더 + 디코더의 구조입니다. 인코더는 입력 문자열을 처리하여 문맥(context)를 출력하고 디코더는 이 문맥을 디코딩하여 목적하는 출력을 내도록 학습합니다.\n",
        "\n",
        "\n",
        "참고 : https://wikidocs.net/24996\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "## 학습 내용\n",
        "\n",
        "다음과 같은 입력문자열에 대하여 출력문자열을 타겟으로 학습한다.\n",
        "\n",
        "```\n",
        "입력문자열 --->  출력문자열\n",
        "'7+81   '  --->  '25  '\n",
        "'583+2  '  --->  '585 '\n",
        "'58+12  '  --->  '70  '\n",
        "'5+85   '  --->  '90  '\n",
        "```\n",
        "\n",
        "결과적으로 입력된 문자열에 대한 덧셈을 수행하는 관계를 학습한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "## 역순 입력\n",
        "\n",
        "성능을 위해서 엽력 문자열을 역순으로 해서 사용한다.\n",
        "\n",
        "```\n",
        "입력문자열 --->  출력문자열\n",
        "'   18+7'  --->  '25  '\n",
        "'  2+385'  --->  '585 '\n",
        "'  21+85'  --->  '70  '\n",
        "'   58+5'  --->  '90  '\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "## 데이터\n",
        "\n",
        "### 입출력 데이터의 길이\n",
        "숫자의 최대 길이(DIGIT)가 3이면 2개의 숫자와 덧셈기호의 최대 길이는 7이다. 출력은 최대 4(3자리 숫자의 합은 최대 4자리)이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### one-hot 인코딩\n",
        "실제 학습에 사용되는 데이터는 각 문자열이 one-hot 인코딩된 값이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### 입력 데이터 모양\n",
        "입력 문자열의 길이는 7이고 각 문자는 12개 길이로 one-hot 인코딩된다.\n",
        "\n",
        "사용되는 문자는 0~9까지 숫자 10개와 덧셈기호와 스페이스 해서 총 12개이다. 입력 문자열의 길이는 7이고 1개의 문자는 길이 12의 one-hot 인코딩된 값이라서 1개 데이터의 모양은 (7,12)이다.\n",
        "\n",
        "입력 문자열이 ' 13+528'이라면 실제 학습에 사용되는 입력값은 다음과 같다.\n",
        "```\n",
        "[[1 0 0 0 0 0 0 0 0 0 0 0]    <--- ' '\n",
        " [0 0 0 1 0 0 0 0 0 0 0 0]    <--- '1'\n",
        " [0 0 0 0 0 1 0 0 0 0 0 0]    <--- '3'\n",
        " [0 1 0 0 0 0 0 0 0 0 0 0]    <--- '+'\n",
        " [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'    \n",
        " [0 0 0 0 1 0 0 0 0 0 0 0]    <--- '2'\n",
        " [0 0 0 0 0 0 0 0 0 0 1 0]]   <--- '8'\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "### 출력 데이터 모양\n",
        "출력 문자열이 '856 '이라면 실제 학습에 사용되는 출력값은 다음과 같다.\n",
        "\n",
        "```        \n",
        "[[0 0 0 0 0 0 0 0 0 0 1 0]    <--- '8'\n",
        " [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'\n",
        " [0 0 0 0 0 0 0 0 1 0 0 0]    <--- '6'\n",
        " [1 0 0 0 0 0 0 0 0 0 0 0]]   <--- ' '     \n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "## 모델\n",
        "\n",
        "### 모델 입출력\n",
        "입력 모양은 (7,12)이고 출력 모양은 (4,12)이다.\n",
        "\n",
        "7은 입력문자열의 최대 길이, 4는 출력문자열의 최대 길이, 그리고 12는 입출력에 사용되는 문자의 갯수가 12라서 one-hot 인코딩된 값의 길이이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### Question-Answer 모델\n",
        "\n",
        "Question-Answer 구조의 모델이다. 질문 무자열이 입력되고 이를 encoder가 인코딩한다. 그리고 그 값을 다시  디코더가 decoding해서 답을 구한다.\n",
        "\n",
        "2개의 LSTM이 각각 인코더, 디코더로 사용된다.\n",
        "\n",
        "<br>\n",
        "\n",
        "다음 구조의 모델을 사용한다.\n",
        "```\n",
        "model = keras.Sequential()\n",
        "model.add(LSTM(..., input_shape=(MAXLEN, 12)) # encoder\n",
        "model.add(RepeatVector(DIGIT+1))\n",
        "model.add(LSTM(..., return_sequences=True)) # decoder\n",
        "model.add(Dense(12, activation=\"softmax\"))\n",
        "```\n",
        "2개의 LSTM이 있고 첫번째가 인코더이고, 두번째가 디코더이다.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "출력층 Dense의 노드 수가 12개 인것은 12개의 문자중에 1개를 선택하기 위한 것이다. 그래서 activation이 softmax이고.\n",
        "\n",
        "전체 모델의 출력은 one-hot 인코딩된 4개의 자리값이어야 한다. 이를 위해 4개의 타임 스텝이 필요하다. 이를 위해 디코더 LSTM앞에 RepeatedVector(4)가 있어서 인코딩된 128 차원의 값을 4번 반복한다.\n",
        "\n",
        "결과적으로 디코더 LSTM은 4개의 값을 출력하고 Dense는 각각 4개의 값에 대한 문자열을 softmax로 출력한다.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ou-ygxNfo-Ec"
      },
      "source": [
        "## 셋업\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5OvBIiWo-Ec"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "\n",
        "TRAINING_SIZE = 50000   # 학습 데이터의 갯수\n",
        "DIGITS = 3              # 덧셈에 사용될 숫자 자리수\n",
        "REVERSE = True          # 입력 문자열의 역순 여부. \n",
        "\n",
        "# 입력 문자열의 최대 길이.\n",
        "# 숫자 2개의 길이 에 '+' 길이 1을 더한 값이다.\n",
        "# 숫자 자리수가 3이면 예를 들어 '345+678'과 같이 최대 길이는 7이다.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dQt0G10o-Eg"
      },
      "source": [
        "## Generate the data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COsdmY5vo-Eg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8688cf86-97ca-49ff-e93f-6dcbdc99747c"
      },
      "source": [
        "# 사용될 문자들. 0~9까지 숫자 10개와 덧셈 '+'와 스페이스 ' '\n",
        "chars = \"0123456789+ \"\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print(\"Generating data...\")\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "\n",
        "    # 임의의 숫자를 만드는 함수\n",
        "    # step1. 최대 자리수를 임의로 선택하고. 최소 1, 최대 3(DIGIT). 선택된 것이 3이라면\n",
        "    # step2. 각 자리의 숫자를 선택하고. '1', '3', '5'\n",
        "    # step3. 각 자리의 숫자를 붙여서 문자열 만들고. '135'\n",
        "    # step4. 만든 문자열을 int로 변환. '135' -> 135\n",
        "    f = lambda: int( # step4\n",
        "        \"\".join( # step3\n",
        "            np.random.choice(list(\"0123456789\")) # step2\n",
        "            for i in range(np.random.randint(1, DIGITS + 1))  # step1\n",
        "        )\n",
        "    )\n",
        "    # 2개의 숫자를 선택했다.\n",
        "    a, b = f(), f()\n",
        "    # a = 123\n",
        "    # b = 45\n",
        "\n",
        "    # 이미 만들었던 문제면 다시 만들자.\n",
        "    # x+y와 y+x의 중복도 체크하기 위해 소팅해서 체크한다.\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "\n",
        "    # 문제 문자열을 만들자\n",
        "    q = \"{}+{}\".format(a, b)\n",
        "    # q = '123+45'\n",
        "\n",
        "    # 문자열 길이가 MAXLEN이 되도록 스페이스를 뒤에 붙인다.\n",
        "    query = q + \" \" * (MAXLEN - len(q))\n",
        "    # query = '123+45 '\n",
        "\n",
        "    # 정답 문자열을 구한다.\n",
        "    ans = str(a + b)\n",
        "    # ans = '168'\n",
        "\n",
        "    # 정답은 최대 DIGITS + 1 길이이다. 최대 길이가 되도록 스페이스를 앞에 붙인다.\n",
        "    ans += \" \" * (DIGITS + 1 - len(ans))\n",
        "    # ans = ' 168'\n",
        "\n",
        "    # 역순으로 처리한다면\n",
        "    # 문제 문자열을 역순으로 한다.\n",
        "    if REVERSE:\n",
        "        # '123+45  '가 '  54+321'가 된다.\n",
        "        query = query[::-1]\n",
        "        # query = ' 54+321'\n",
        "\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "\n",
        "print(\"Total questions:\", len(questions))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating data...\n",
            "Total questions: 50000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmLHQod8nlcU",
        "outputId": "5faf21ad-4230-4884-acb8-69fe87171df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(questions[:5])\n",
        "print(expected[:5])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['  857+7', '  8+289', '    7+1', ' 338+08', '  8+708']\n",
            "['765 ', '990 ', '8   ', '913 ', '815 ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7eFX6jIyrnx"
      },
      "source": [
        "# 데이터를 벡터화 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO8skDb7p3f"
      },
      "source": [
        "인코딩 디코딩을 위한 유틸 클래스 CharacterTable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sqn4LUtHyi46"
      },
      "source": [
        "class CharacterTable:\n",
        "    \"\"\" 특정 문자열 집합을 가지고:\n",
        "    + one-hot 인코딩\n",
        "    + one-hot된 혹은 숫자로 인코딩된 값을 원 무자열로 디코딩\n",
        "    + softmax 같은 확율 벡터를 원 문자열로 디코딩\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"문자 테이블을 초기화\n",
        "        # Arguments\n",
        "            chars: 입력에 사용되는 문자들\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        # self.char_indices = { ' ': 0, \n",
        "        #                       '+': 1,\n",
        "        #                       '0': 2, \n",
        "        #                       '1': 3, \n",
        "        #                       ...\n",
        "        #                       '9': 11}\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "        # self.indices_char = { 0: ' ', \n",
        "        #                       1: '+', \n",
        "        #                       2: '0', \n",
        "        #                       3: '1', \n",
        "        #                       ...\n",
        "        #                       11: '9'}\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"전달된 문자열 C를 One-hot 인코딩 한다.\n",
        "        # Arguments\n",
        "            C: 인코딩할 문자열. 예 ' 123+45'\n",
        "            num_rows: 반환될 행 수. 입력 길이와 관계없이 반환되는 행 수를 동일하기 위해 사용된다.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C): # 각 자리 문자마다\n",
        "            x[i, self.char_indices[c]] = 1 # 각 줄의 index만 1로 만든다. one-hot encoding한다.\n",
        "\n",
        "        # 반환되는 x는 num_rows의 행이고, 각 행은 각 문자에 대한 one-hot 인코딩된 값이다.\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"전달된 벡터값 혹은 2D 배열을 해당 문자열로 디코딩한다.\n",
        "        # Arguments\n",
        "            x: 벡터 혹은 one-hot 인코딩된 2D 배열 혹은 softmax된 확률 2D 배열 \n",
        "               혹은 calc_argmax=False일 때는 문자 인덱스 벡터\n",
        "            calc_argmax: 최대 값의 index 값을 찾을 지 여부. default는 True.\n",
        "        \"\"\"\n",
        "\n",
        "        # one-hot 인코딩 혹은 softmax 값이면 최대 index를 구한다.\n",
        "        if calc_argmax:\n",
        "            # x = [ [0 0 0 0 1 0 0 0 0 0 0 0]\n",
        "            #       [0 0 0 0 0 1 0 0 0 0 0 0]\n",
        "            #       [0 0 0 0 0 0 1 0 0 0 0 0]]\n",
        "            x = x.argmax(axis=-1)\n",
        "            # x = [ 4 5 6 ]\n",
        "        return \"\".join(self.indices_char[x] for x in x)\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVYzei1I7XZb"
      },
      "source": [
        "ctable = CharacterTable(chars)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_fJZA7b7Y-e"
      },
      "source": [
        "다음은 인코딩, 디코딩 예이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwm40AA-r4tt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c51ec4-25c6-4575-98e7-8c7c18d1276a"
      },
      "source": [
        "# 원 문자열\n",
        "org_str = ' 123+45'\n",
        "print(f\"org_str='{org_str}'\")\n",
        "\n",
        "# 인코딩된 ㄱ밧\n",
        "encoded = ctable.encode(org_str, MAXLEN)\n",
        "# [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- ' '\n",
        "#  [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- '1'\n",
        "#  [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]      <--- '2'\n",
        "#  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]      <--- '3'\n",
        "#  [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]      <--- '+'\n",
        "#  [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]      <--- '4'\n",
        "#  [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]     <--- '5'\n",
        "print(f\"encoded=\\n{encoded}\")\n",
        "print(f\"encoded.shape={encoded.shape}\")\n",
        "\n",
        "# 다시 디코딩해서 원복한 값\n",
        "decoded = ctable.decode(encoded)\n",
        "print(f\"decoded='{decoded}'\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "org_str=' 123+45'\n",
            "encoded=\n",
            "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n",
            "encoded.shape=(7, 12)\n",
            "decoded=' 123+45'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1QHj3MDo-Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "422c0ad9-0a6b-4ceb-9055-1ee62eaf6376"
      },
      "source": [
        "print(\"Vectorization...\")\n",
        "\n",
        "# DIGITS = 3\n",
        "# MAXLEN = DIGITS + 1 + DIGITS\n",
        "# chars = '0123456789+'\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "# x 데이터 한개의 모양은 (7, 12)  # 7 글자, 12개 글자 종류(0~9, '+', ' ')\n",
        "# y 데이터 한개의 모양은 (3, 12)  # 3 글자, 12개 글자 종류(0~9, '+', ' ')\n",
        "\n",
        "# x의 뒷쪽은 대부분 큰 자리수의 문제이다.\n",
        "# 이를 커버하기 위해 섞는다.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# 명시적으로 10%는 학습에 사용되지 않는 validation 데이터로 딸 둔다.\n",
        "# 역자주 : 보통 데이터는 train/validation/test로 분리합니다. \n",
        "# 학습 시에 사용하지 않는 것은 test 데이터인데 저자가 혼동한 듯 합니다.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "print(\"Validation Data:\")\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorization...\n",
            "Training Data:\n",
            "(45000, 7, 12)\n",
            "(45000, 4, 12)\n",
            "Validation Data:\n",
            "(5000, 7, 12)\n",
            "(5000, 4, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMq2Om3w8qNy"
      },
      "source": [
        "준비된 데이터는 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41UcKyn-q5Ez",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c131be1-a819-429c-c786-55e40406b38e"
      },
      "source": [
        "print(\"x_train[0]\\n\", x_train[0].astype(np.int))\n",
        "print()\n",
        "print(\"y_train[0]\\n\", y_train[0].astype(np.int))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train[0]\n",
            " [[1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0]]\n",
            "\n",
            "y_train[0]\n",
            " [[0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa-gM3Nq84XK"
      },
      "source": [
        "준비된 데이터는 인코딩된 값들입니다. 디코딩에서 본 원 문자열은 다음과 같습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw_RFfJvrGFM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636b4ee3-b734-429d-bb3a-b030663b0b61"
      },
      "source": [
        "for i in range(10):\n",
        "  print(\"'\"+ctable.decode(x_train[i])+\"' -> '\"+ctable.decode(y_train[i])+\"'\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "' 15+857' -> '809 '\n",
            "'131+293' -> '523 '\n",
            "'  4+887' -> '792 '\n",
            "' 64+237' -> '778 '\n",
            "' 988+62' -> '915 '\n",
            "' 84+641' -> '194 '\n",
            "'818+866' -> '1486'\n",
            "' 824+71' -> '445 '\n",
            "'227+045' -> '1262'\n",
            "' 077+39' -> '863 '\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MkDWbri9EAd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2liXf2Yo-En"
      },
      "source": [
        "## Build the model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP8Vw2aNtzPn"
      },
      "source": [
        "# 모델 정의\n",
        "\n",
        "LSTM을 사용. 모델은 다음과 같다.\n",
        "\n",
        "```\n",
        "model.add(layers.LSTM(..., input_shape=(7, 12)) # 7:입력 최대 길이, 12:문자 종류 수\n",
        "model.add(RepeatVector(4)) # 4:숫자 최대길이 3+1. 4자리가 되도록 값을 반복한다.\n",
        "model.add(LSTM(..., return_sequences=True)\n",
        "model.add(Dense(12, activation=\"softmax\")) # 12:문자 종류 수\n",
        "```\n",
        "\n",
        "각 층의 출력은 다음과 같다.\n",
        "```\n",
        "LSTM            (None, 128)\n",
        "RepeatVector    (None, 4, 128)    # [1,2] -> [ [1,2], [1,2], [1,2], [1,2] ]\n",
        "LSTM            (None, 4, 128)\n",
        "Dense           (None, 4, 12)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFO2GZHdos1e"
      },
      "source": [
        "```\n",
        "model = keras.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "model.add(layers.LSTM(128, return_sequences=True))\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "\n",
        "\n",
        "lstm (LSTM)                  (None, 128)               72192     \n",
        "repeat_vector (RepeatVector) (None, 4, 128)            0         \n",
        "lstm_1 (LSTM)                (None, 4, 128)            131584    \n",
        "dense (Dense)                (None, 4, 12)             1548      \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUr-sqkoo-En",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "727c91e2-b48b-476c-f355-b4af05efc869"
      },
      "source": [
        "print(\"Build model...\")\n",
        "num_layers = 1  # Try to add more LSTM layers!\n",
        "\n",
        "model = keras.Sequential()\n",
        "# 입력 순차열을 LSTM을 사용하여 128길이의 출력으로 인코딩한다.\n",
        "# 주의 : 입력 길이가 변하는 상황이면 input_shape=(None, num_feature)로 한다.\n",
        "model.add(layers.LSTM(128, input_shape=(MAXLEN, len(chars))))\n",
        "\n",
        "# 디코더 입력으로 인코딩된 값을 각 시간 스텝(출력 자리) 수 만큼 반복해 준다.\n",
        "# 출력 최대 길이인 DIGIT+1만큼 반복한다. \n",
        "# DIGIT가 3일때 최대 출력값은 999+999=1998이다.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "\n",
        "# 디코더 RNN은 한 층 혹은 여러층일 수 있다.\n",
        "for _ in range(num_layers):\n",
        "    # return_sequences을 True로 놓아 마지막 스텝뿐 아니라 전체 스텝의 출력값을 \n",
        "    # (num_samples, timesteps, output_dimc) 모양으로 출력한다.\n",
        "    # 이것은 다음 층에서 사용되는 TimeDistributed가 입력의 첫 차원이 시간 스텝이어야 하기 때문이다.\n",
        "    #\n",
        "    # 역자주 : TensorFlow 2.0이전에는 model.add(TimeDistributed(Dense(..)))의 형태로 쓰여야 했다.\n",
        "    # 주석에서 TimeDistributed에 관련된 것은 아마도 코드를 업데이트 하면서 주석은 업데이트 하지 않은 것 같다.\n",
        "    model.add(layers.LSTM(128, return_sequences=True))\n",
        "\n",
        "# 각 스텝의 출력에 대한 문자열을 선택하도록 한다.\n",
        "model.add(layers.Dense(len(chars), activation=\"softmax\"))\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_2 (LSTM)                (None, 128)               72192     \n",
            "_________________________________________________________________\n",
            "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 4, 128)            131584    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4, 12)             1548      \n",
            "=================================================================\n",
            "Total params: 205,324\n",
            "Trainable params: 205,324\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5L-7r5Yo-Er"
      },
      "source": [
        "## 모델 학습\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-Zav9Fro-Er",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7e36a1d-28e8-430f-d453-0b3fa497adbd"
      },
      "source": [
        "epochs = 30\n",
        "batch_size = 32\n",
        "\n",
        "# 학습을 실행하고, 각 학습 중에 validation 데이터에 대한 결과를 보여준다.\n",
        "for epoch in range(1, epochs):\n",
        "    print()\n",
        "    print(\"Iteration\", epoch)\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=1,\n",
        "        validation_data=(x_val, y_val),\n",
        "    )\n",
        "\n",
        "    # 10개 랜덤 샘플을 골라 결과를 보여준다.\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        # rowx = [[[1 0 0 0 0 0 0 0 0 0 0 0]    <--- ' '\n",
        "        #          [0 0 0 1 0 0 0 0 0 0 0 0]    <--- '1'\n",
        "        #          [0 0 0 0 0 1 0 0 0 0 0 0]    <--- '3'\n",
        "        #          [0 1 0 0 0 0 0 0 0 0 0 0]    <--- '+'\n",
        "        #          [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'    \n",
        "        #          [0 0 0 0 1 0 0 0 0 0 0 0]    <--- '2'\n",
        "        #          [0 0 0 0 0 0 0 0 0 0 1 0]]]  <--- '8'\n",
        "        #                                            ' 13+528'\n",
        "        #\n",
        "        # rowy = [[[0 0 0 0 0 0 0 0 0 0 1 0]    <--- '8'\n",
        "        #          [0 0 0 0 0 0 0 1 0 0 0 0]    <--- '5'\n",
        "        #          [0 0 0 0 0 0 0 0 1 0 0 0]    <--- '6'\n",
        "        #          [1 0 0 0 0 0 0 0 0 0 0 0]]]  <--- ' '     \n",
        "        #                                            '856 '   \n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        # preds = [[10  7  8  0]]\n",
        "        # 요 preds 값은 CharacterTable에 정의된 indices_char의 키 값에 해당한다.\n",
        "        #   {0: ' ',   # <------\n",
        "        #    1: '+', \n",
        "        #    ...\n",
        "        #    7: '5'    # <------\n",
        "        #    8: '6'    # <------\n",
        "        #    9: '7'\n",
        "        #    10: '8'   # <------\n",
        "        #    11: '9'}        \n",
        "\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        #  q       = ' 13+528'  <--- '825+31 '가 reverse된 것.\n",
        "        #  correct = '856 '\n",
        "        #  guess   = '856 '        \n",
        "        \n",
        "        # REVERSE = True\n",
        "        print(\"Q\", q[::-1] if REVERSE else q, end=\" \")\n",
        "        # Q 825+31\n",
        "        print(\"T\", correct, end=\" \")\n",
        "        # T 856  \n",
        "        if correct == guess:\n",
        "            print(\"☑ \" + guess)\n",
        "        else:\n",
        "            print(\"☒ \" + guess)\n",
        "        # ☑ 856             \n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Iteration 1\n",
            "1407/1407 [==============================] - 14s 6ms/step - loss: 1.7609 - accuracy: 0.3554 - val_loss: 1.6113 - val_accuracy: 0.4028\n",
            "Q 75+844  T 919  ☒ 844 \n",
            "Q 290+726 T 1016 ☒ 801 \n",
            "Q 111+857 T 968  ☒ 906 \n",
            "Q 14+338  T 352  ☒ 33  \n",
            "Q 41+834  T 875  ☒ 449 \n",
            "Q 740+571 T 1311 ☒ 1004\n",
            "Q 582+88  T 670  ☒ 899 \n",
            "Q 467+80  T 547  ☒ 779 \n",
            "Q 922+8   T 930  ☒ 199 \n",
            "Q 1+491   T 492  ☒ 111 \n",
            "\n",
            "Iteration 2\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 1.3461 - accuracy: 0.4963 - val_loss: 1.1747 - val_accuracy: 0.5646\n",
            "Q 987+73  T 1060 ☒ 1054\n",
            "Q 74+856  T 930  ☒ 945 \n",
            "Q 37+799  T 836  ☒ 824 \n",
            "Q 12+379  T 391  ☒ 386 \n",
            "Q 832+0   T 832  ☒ 834 \n",
            "Q 154+28  T 182  ☒ 184 \n",
            "Q 755+4   T 759  ☒ 751 \n",
            "Q 83+356  T 439  ☒ 424 \n",
            "Q 682+700 T 1382 ☒ 1305\n",
            "Q 662+482 T 1144 ☒ 1105\n",
            "\n",
            "Iteration 3\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 1.0581 - accuracy: 0.6074 - val_loss: 0.9755 - val_accuracy: 0.6403\n",
            "Q 7+919   T 926  ☒ 924 \n",
            "Q 18+2    T 20   ☒ 15  \n",
            "Q 507+48  T 555  ☒ 568 \n",
            "Q 132+9   T 141  ☒ 149 \n",
            "Q 200+219 T 419  ☒ 334 \n",
            "Q 25+397  T 422  ☒ 437 \n",
            "Q 26+539  T 565  ☒ 568 \n",
            "Q 26+327  T 353  ☒ 355 \n",
            "Q 941+515 T 1456 ☒ 1486\n",
            "Q 81+499  T 580  ☒ 576 \n",
            "\n",
            "Iteration 4\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.9139 - accuracy: 0.6614 - val_loss: 0.8774 - val_accuracy: 0.6714\n",
            "Q 518+627 T 1145 ☒ 1166\n",
            "Q 1+432   T 433  ☒ 434 \n",
            "Q 438+5   T 443  ☒ 440 \n",
            "Q 32+868  T 900  ☑ 900 \n",
            "Q 686+717 T 1403 ☒ 1300\n",
            "Q 65+52   T 117  ☒ 110 \n",
            "Q 271+271 T 542  ☒ 520 \n",
            "Q 206+6   T 212  ☒ 219 \n",
            "Q 49+347  T 396  ☒ 490 \n",
            "Q 21+378  T 399  ☒ 400 \n",
            "\n",
            "Iteration 5\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7996 - accuracy: 0.7041 - val_loss: 0.7414 - val_accuracy: 0.7272\n",
            "Q 680+16  T 696  ☒ 697 \n",
            "Q 804+9   T 813  ☑ 813 \n",
            "Q 52+425  T 477  ☒ 473 \n",
            "Q 80+932  T 1012 ☒ 1010\n",
            "Q 77+94   T 171  ☒ 178 \n",
            "Q 509+5   T 514  ☒ 513 \n",
            "Q 22+544  T 566  ☒ 564 \n",
            "Q 25+187  T 212  ☒ 216 \n",
            "Q 646+7   T 653  ☑ 653 \n",
            "Q 252+741 T 993  ☒ 990 \n",
            "\n",
            "Iteration 6\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.7031 - accuracy: 0.7432 - val_loss: 0.6482 - val_accuracy: 0.7668\n",
            "Q 893+52  T 945  ☒ 940 \n",
            "Q 698+97  T 795  ☒ 787 \n",
            "Q 401+817 T 1218 ☒ 1229\n",
            "Q 3+47    T 50   ☒ 48  \n",
            "Q 287+429 T 716  ☒ 712 \n",
            "Q 121+551 T 672  ☒ 678 \n",
            "Q 3+49    T 52   ☒ 40  \n",
            "Q 312+244 T 556  ☑ 556 \n",
            "Q 658+25  T 683  ☒ 680 \n",
            "Q 79+86   T 165  ☒ 162 \n",
            "\n",
            "Iteration 7\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.5864 - accuracy: 0.7886 - val_loss: 0.4823 - val_accuracy: 0.8278\n",
            "Q 647+73  T 720  ☑ 720 \n",
            "Q 175+699 T 874  ☒ 883 \n",
            "Q 260+46  T 306  ☒ 307 \n",
            "Q 81+550  T 631  ☒ 633 \n",
            "Q 507+22  T 529  ☒ 539 \n",
            "Q 939+64  T 1003 ☒ 1012\n",
            "Q 378+4   T 382  ☑ 382 \n",
            "Q 52+217  T 269  ☒ 279 \n",
            "Q 608+87  T 695  ☒ 697 \n",
            "Q 895+254 T 1149 ☒ 1141\n",
            "\n",
            "Iteration 8\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.3530 - accuracy: 0.8861 - val_loss: 0.2682 - val_accuracy: 0.9230\n",
            "Q 33+748  T 781  ☑ 781 \n",
            "Q 28+388  T 416  ☒ 415 \n",
            "Q 73+126  T 199  ☑ 199 \n",
            "Q 99+29   T 128  ☑ 128 \n",
            "Q 461+476 T 937  ☑ 937 \n",
            "Q 83+910  T 993  ☒ 994 \n",
            "Q 64+790  T 854  ☑ 854 \n",
            "Q 546+741 T 1287 ☒ 1288\n",
            "Q 12+618  T 630  ☒ 620 \n",
            "Q 55+540  T 595  ☑ 595 \n",
            "\n",
            "Iteration 9\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.2136 - accuracy: 0.9441 - val_loss: 0.1782 - val_accuracy: 0.9475\n",
            "Q 64+49   T 113  ☑ 113 \n",
            "Q 674+83  T 757  ☑ 757 \n",
            "Q 432+790 T 1222 ☒ 1223\n",
            "Q 649+225 T 874  ☑ 874 \n",
            "Q 85+413  T 498  ☑ 498 \n",
            "Q 886+961 T 1847 ☒ 1837\n",
            "Q 911+297 T 1208 ☑ 1208\n",
            "Q 5+771   T 776  ☑ 776 \n",
            "Q 40+772  T 812  ☒ 813 \n",
            "Q 8+130   T 138  ☑ 138 \n",
            "\n",
            "Iteration 10\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.1358 - accuracy: 0.9691 - val_loss: 0.1085 - val_accuracy: 0.9759\n",
            "Q 488+85  T 573  ☑ 573 \n",
            "Q 562+142 T 704  ☑ 704 \n",
            "Q 405+16  T 421  ☑ 421 \n",
            "Q 11+864  T 875  ☑ 875 \n",
            "Q 111+38  T 149  ☑ 149 \n",
            "Q 80+925  T 1005 ☑ 1005\n",
            "Q 24+17   T 41   ☒ 31  \n",
            "Q 18+53   T 71   ☑ 71  \n",
            "Q 32+850  T 882  ☑ 882 \n",
            "Q 11+896  T 907  ☑ 907 \n",
            "\n",
            "Iteration 11\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0852 - accuracy: 0.9824 - val_loss: 0.0913 - val_accuracy: 0.9776\n",
            "Q 979+2   T 981  ☑ 981 \n",
            "Q 62+861  T 923  ☑ 923 \n",
            "Q 2+525   T 527  ☑ 527 \n",
            "Q 99+67   T 166  ☑ 166 \n",
            "Q 663+36  T 699  ☑ 699 \n",
            "Q 477+68  T 545  ☑ 545 \n",
            "Q 78+525  T 603  ☑ 603 \n",
            "Q 504+607 T 1111 ☑ 1111\n",
            "Q 318+3   T 321  ☑ 321 \n",
            "Q 847+2   T 849  ☑ 849 \n",
            "\n",
            "Iteration 12\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0641 - accuracy: 0.9861 - val_loss: 0.0437 - val_accuracy: 0.9919\n",
            "Q 11+896  T 907  ☑ 907 \n",
            "Q 611+756 T 1367 ☑ 1367\n",
            "Q 4+712   T 716  ☑ 716 \n",
            "Q 294+31  T 325  ☑ 325 \n",
            "Q 794+3   T 797  ☑ 797 \n",
            "Q 920+773 T 1693 ☑ 1693\n",
            "Q 210+69  T 279  ☑ 279 \n",
            "Q 999+964 T 1963 ☑ 1963\n",
            "Q 551+546 T 1097 ☑ 1097\n",
            "Q 567+23  T 590  ☑ 590 \n",
            "\n",
            "Iteration 13\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0632 - accuracy: 0.9845 - val_loss: 0.0366 - val_accuracy: 0.9937\n",
            "Q 123+997 T 1120 ☒ 1110\n",
            "Q 562+74  T 636  ☑ 636 \n",
            "Q 8+596   T 604  ☑ 604 \n",
            "Q 361+750 T 1111 ☑ 1111\n",
            "Q 54+336  T 390  ☑ 390 \n",
            "Q 80+39   T 119  ☑ 119 \n",
            "Q 3+682   T 685  ☑ 685 \n",
            "Q 776+78  T 854  ☑ 854 \n",
            "Q 644+12  T 656  ☑ 656 \n",
            "Q 72+130  T 202  ☑ 202 \n",
            "\n",
            "Iteration 14\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0433 - accuracy: 0.9898 - val_loss: 0.0237 - val_accuracy: 0.9967\n",
            "Q 68+209  T 277  ☑ 277 \n",
            "Q 53+159  T 212  ☑ 212 \n",
            "Q 341+748 T 1089 ☑ 1089\n",
            "Q 56+105  T 161  ☑ 161 \n",
            "Q 469+536 T 1005 ☑ 1005\n",
            "Q 991+412 T 1403 ☑ 1403\n",
            "Q 637+18  T 655  ☑ 655 \n",
            "Q 802+63  T 865  ☑ 865 \n",
            "Q 548+339 T 887  ☑ 887 \n",
            "Q 773+28  T 801  ☑ 801 \n",
            "\n",
            "Iteration 15\n",
            "1407/1407 [==============================] - 9s 6ms/step - loss: 0.0318 - accuracy: 0.9932 - val_loss: 0.0235 - val_accuracy: 0.9953\n",
            "Q 896+833 T 1729 ☑ 1729\n",
            "Q 67+295  T 362  ☑ 362 \n",
            "Q 491+39  T 530  ☑ 530 \n",
            "Q 45+802  T 847  ☑ 847 \n",
            "Q 35+89   T 124  ☑ 124 \n",
            "Q 2+436   T 438  ☑ 438 \n",
            "Q 14+732  T 746  ☑ 746 \n",
            "Q 958+15  T 973  ☑ 973 \n",
            "Q 2+994   T 996  ☑ 996 \n",
            "Q 329+9   T 338  ☑ 338 \n",
            "\n",
            "Iteration 16\n",
            "1407/1407 [==============================] - 9s 6ms/step - loss: 0.0307 - accuracy: 0.9923 - val_loss: 0.0231 - val_accuracy: 0.9942\n",
            "Q 997+13  T 1010 ☑ 1010\n",
            "Q 327+438 T 765  ☑ 765 \n",
            "Q 5+902   T 907  ☑ 907 \n",
            "Q 866+411 T 1277 ☑ 1277\n",
            "Q 937+896 T 1833 ☑ 1833\n",
            "Q 677+9   T 686  ☑ 686 \n",
            "Q 63+497  T 560  ☑ 560 \n",
            "Q 71+560  T 631  ☑ 631 \n",
            "Q 428+401 T 829  ☑ 829 \n",
            "Q 72+56   T 128  ☑ 128 \n",
            "\n",
            "Iteration 17\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0335 - accuracy: 0.9917 - val_loss: 0.0131 - val_accuracy: 0.9982\n",
            "Q 199+516 T 715  ☑ 715 \n",
            "Q 83+33   T 116  ☑ 116 \n",
            "Q 941+23  T 964  ☑ 964 \n",
            "Q 245+515 T 760  ☑ 760 \n",
            "Q 71+70   T 141  ☑ 141 \n",
            "Q 746+25  T 771  ☑ 771 \n",
            "Q 125+25  T 150  ☑ 150 \n",
            "Q 936+710 T 1646 ☑ 1646\n",
            "Q 600+703 T 1303 ☑ 1303\n",
            "Q 647+929 T 1576 ☑ 1576\n",
            "\n",
            "Iteration 18\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0363 - accuracy: 0.9904 - val_loss: 0.0131 - val_accuracy: 0.9977\n",
            "Q 742+15  T 757  ☑ 757 \n",
            "Q 472+995 T 1467 ☑ 1467\n",
            "Q 551+258 T 809  ☑ 809 \n",
            "Q 479+520 T 999  ☑ 999 \n",
            "Q 647+73  T 720  ☑ 720 \n",
            "Q 51+70   T 121  ☑ 121 \n",
            "Q 758+320 T 1078 ☑ 1078\n",
            "Q 2+795   T 797  ☑ 797 \n",
            "Q 379+193 T 572  ☑ 572 \n",
            "Q 3+557   T 560  ☑ 560 \n",
            "\n",
            "Iteration 19\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0275 - accuracy: 0.9930 - val_loss: 0.0093 - val_accuracy: 0.9987\n",
            "Q 42+565  T 607  ☑ 607 \n",
            "Q 788+66  T 854  ☑ 854 \n",
            "Q 571+35  T 606  ☑ 606 \n",
            "Q 79+3    T 82   ☑ 82  \n",
            "Q 346+534 T 880  ☑ 880 \n",
            "Q 98+159  T 257  ☑ 257 \n",
            "Q 52+709  T 761  ☑ 761 \n",
            "Q 494+86  T 580  ☑ 580 \n",
            "Q 782+6   T 788  ☑ 788 \n",
            "Q 798+619 T 1417 ☑ 1417\n",
            "\n",
            "Iteration 20\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0298 - accuracy: 0.9920 - val_loss: 0.0171 - val_accuracy: 0.9962\n",
            "Q 27+349  T 376  ☑ 376 \n",
            "Q 921+3   T 924  ☑ 924 \n",
            "Q 967+95  T 1062 ☑ 1062\n",
            "Q 5+30    T 35   ☑ 35  \n",
            "Q 442+69  T 511  ☑ 511 \n",
            "Q 588+29  T 617  ☑ 617 \n",
            "Q 55+806  T 861  ☑ 861 \n",
            "Q 997+11  T 1008 ☑ 1008\n",
            "Q 987+30  T 1017 ☑ 1017\n",
            "Q 48+458  T 506  ☑ 506 \n",
            "\n",
            "Iteration 21\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0145 - accuracy: 0.9967 - val_loss: 0.0295 - val_accuracy: 0.9906\n",
            "Q 77+14   T 91   ☑ 91  \n",
            "Q 303+85  T 388  ☑ 388 \n",
            "Q 532+27  T 559  ☑ 559 \n",
            "Q 0+311   T 311  ☑ 311 \n",
            "Q 24+56   T 80   ☑ 80  \n",
            "Q 80+248  T 328  ☑ 328 \n",
            "Q 68+981  T 1049 ☒ 1059\n",
            "Q 55+202  T 257  ☑ 257 \n",
            "Q 53+911  T 964  ☑ 964 \n",
            "Q 12+379  T 391  ☑ 391 \n",
            "\n",
            "Iteration 22\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0283 - accuracy: 0.9926 - val_loss: 0.0250 - val_accuracy: 0.9929\n",
            "Q 859+65  T 924  ☑ 924 \n",
            "Q 25+593  T 618  ☑ 618 \n",
            "Q 77+69   T 146  ☑ 146 \n",
            "Q 21+71   T 92   ☑ 92  \n",
            "Q 7+214   T 221  ☑ 221 \n",
            "Q 31+736  T 767  ☑ 767 \n",
            "Q 125+187 T 312  ☑ 312 \n",
            "Q 32+787  T 819  ☑ 819 \n",
            "Q 1+485   T 486  ☑ 486 \n",
            "Q 164+12  T 176  ☑ 176 \n",
            "\n",
            "Iteration 23\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.0188 - val_accuracy: 0.9945\n",
            "Q 727+59  T 786  ☑ 786 \n",
            "Q 7+879   T 886  ☑ 886 \n",
            "Q 91+793  T 884  ☑ 884 \n",
            "Q 870+28  T 898  ☑ 898 \n",
            "Q 5+30    T 35   ☑ 35  \n",
            "Q 329+749 T 1078 ☑ 1078\n",
            "Q 657+69  T 726  ☑ 726 \n",
            "Q 66+69   T 135  ☑ 135 \n",
            "Q 68+698  T 766  ☑ 766 \n",
            "Q 5+566   T 571  ☑ 571 \n",
            "\n",
            "Iteration 24\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0274 - accuracy: 0.9922 - val_loss: 0.0070 - val_accuracy: 0.9987\n",
            "Q 406+6   T 412  ☑ 412 \n",
            "Q 595+81  T 676  ☑ 676 \n",
            "Q 55+25   T 80   ☑ 80  \n",
            "Q 401+109 T 510  ☑ 510 \n",
            "Q 29+535  T 564  ☑ 564 \n",
            "Q 78+645  T 723  ☑ 723 \n",
            "Q 2+690   T 692  ☑ 692 \n",
            "Q 0+246   T 246  ☑ 246 \n",
            "Q 46+37   T 83   ☑ 83  \n",
            "Q 4+338   T 342  ☑ 342 \n",
            "\n",
            "Iteration 25\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0244 - accuracy: 0.9932 - val_loss: 0.0098 - val_accuracy: 0.9977\n",
            "Q 3+917   T 920  ☑ 920 \n",
            "Q 188+106 T 294  ☑ 294 \n",
            "Q 547+420 T 967  ☑ 967 \n",
            "Q 838+33  T 871  ☑ 871 \n",
            "Q 500+46  T 546  ☑ 546 \n",
            "Q 829+79  T 908  ☑ 908 \n",
            "Q 357+52  T 409  ☑ 409 \n",
            "Q 380+8   T 388  ☑ 388 \n",
            "Q 90+480  T 570  ☑ 570 \n",
            "Q 62+354  T 416  ☑ 416 \n",
            "\n",
            "Iteration 26\n",
            "1407/1407 [==============================] - 9s 6ms/step - loss: 0.0038 - accuracy: 0.9998 - val_loss: 0.0038 - val_accuracy: 0.9995\n",
            "Q 128+96  T 224  ☑ 224 \n",
            "Q 833+54  T 887  ☑ 887 \n",
            "Q 36+61   T 97   ☑ 97  \n",
            "Q 585+92  T 677  ☑ 677 \n",
            "Q 260+266 T 526  ☑ 526 \n",
            "Q 2+632   T 634  ☑ 634 \n",
            "Q 64+143  T 207  ☑ 207 \n",
            "Q 59+26   T 85   ☑ 85  \n",
            "Q 0+873   T 873  ☑ 873 \n",
            "Q 575+4   T 579  ☑ 579 \n",
            "\n",
            "Iteration 27\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0256 - accuracy: 0.9933 - val_loss: 0.0127 - val_accuracy: 0.9972\n",
            "Q 114+158 T 272  ☑ 272 \n",
            "Q 57+898  T 955  ☑ 955 \n",
            "Q 677+9   T 686  ☑ 686 \n",
            "Q 69+16   T 85   ☑ 85  \n",
            "Q 48+13   T 61   ☑ 61  \n",
            "Q 816+11  T 827  ☑ 827 \n",
            "Q 2+44    T 46   ☑ 46  \n",
            "Q 477+68  T 545  ☑ 545 \n",
            "Q 892+16  T 908  ☑ 908 \n",
            "Q 426+447 T 873  ☑ 873 \n",
            "\n",
            "Iteration 28\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0083 - accuracy: 0.9982 - val_loss: 0.0130 - val_accuracy: 0.9972\n",
            "Q 513+211 T 724  ☑ 724 \n",
            "Q 357+65  T 422  ☑ 422 \n",
            "Q 127+15  T 142  ☑ 142 \n",
            "Q 794+26  T 820  ☑ 820 \n",
            "Q 663+785 T 1448 ☑ 1448\n",
            "Q 359+717 T 1076 ☑ 1076\n",
            "Q 102+185 T 287  ☑ 287 \n",
            "Q 21+1    T 22   ☑ 22  \n",
            "Q 9+724   T 733  ☑ 733 \n",
            "Q 999+678 T 1677 ☑ 1677\n",
            "\n",
            "Iteration 29\n",
            "1407/1407 [==============================] - 8s 6ms/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 0.0074 - val_accuracy: 0.9985\n",
            "Q 710+474 T 1184 ☑ 1184\n",
            "Q 9+144   T 153  ☑ 153 \n",
            "Q 680+215 T 895  ☑ 895 \n",
            "Q 190+47  T 237  ☑ 237 \n",
            "Q 59+163  T 222  ☑ 222 \n",
            "Q 972+289 T 1261 ☑ 1261\n",
            "Q 430+90  T 520  ☑ 520 \n",
            "Q 10+664  T 674  ☑ 674 \n",
            "Q 215+7   T 222  ☑ 222 \n",
            "Q 38+247  T 285  ☑ 285 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3AO_JLlo-Eu"
      },
      "source": [
        "대략 30 epoch 뒤에 99+% 정도의 validation 정확도를 보인다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx4wwMdOBGCg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}