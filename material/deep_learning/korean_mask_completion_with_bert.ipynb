{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "korean_mask_completion_with_bert.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyz8OyCRKRz8"
      },
      "source": [
        "# Bert를 사용한 괄호 단어 찾기. 사용자 vocab을 사용한\n",
        "\n",
        "2개의 채팅 문답에서 한 부분을 괄호 치고, 이 괄호 친 부분의 단어를 예측.\n",
        "\n",
        "```\n",
        " 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
        "\n",
        " 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 (??)의 일부니까요.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBC-mz4q4kIr"
      },
      "source": [
        "copy from https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/blob/master/7.PRETRAIN_METHOD/7.2.2.bert_finetune_KorNLI.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFwbg7k1KZrK"
      },
      "source": [
        "# 필요 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTUt2OoT04Kh",
        "outputId": "cf1a3fc1-9706-4d3a-dc2e-d7c1d935e769"
      },
      "source": [
        "!pip install transformers==3.0.2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.0)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 36.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 31.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVofHG0eh40t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b5a45d-ce7f-47c8-d88a-18571e3b4685"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 56.4 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOH1dbiHh5_S"
      },
      "source": [
        "# 셋업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-FHX9tOK4pV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertModel\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8GjpDqK7Lm"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "SEQ_LENGTH = 128\n",
        "BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
        "CUSTOM_VOCAB_FILE = 'custom_vocab.txt'\n",
        "MAX_VOCAB_SIZE = 10000*2"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104nTDhQLQmP"
      },
      "source": [
        "# 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bobHiFxmBN01"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjJehKcMOA9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec3d0a22-be43-4b57-d902-4d7ae5938fb0"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-20 12:45:37--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘ChatbotData.csv’\n",
            "\n",
            "\rChatbotData.csv       0%[                    ]       0  --.-KB/s               \rChatbotData.csv     100%[===================>] 868.99K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2021-11-20 12:45:38 (17.4 MB/s) - ‘ChatbotData.csv’ saved [889842/889842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ILWTRSQBWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1778661a-c67a-4cd1-8f3b-f036095a2bdb"
      },
      "source": [
        "!head ChatbotData.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q,A,label\r\n",
            "12시 땡!,하루가 또 가네요.,0\r\n",
            "1지망 학교 떨어졌어,위로해 드립니다.,0\r\n",
            "3박4일 놀러가고 싶다,여행은 언제나 좋죠.,0\r\n",
            "3박4일 정도 놀러가고 싶다,여행은 언제나 좋죠.,0\r\n",
            "PPL 심하네,눈살이 찌푸려지죠.,0\r\n",
            "SD카드 망가졌어,다시 새로 사는 게 마음 편해요.,0\r\n",
            "SD카드 안돼,다시 새로 사는 게 마음 편해요.,0\r\n",
            "SNS 맞팔 왜 안하지ㅠㅠ,잘 모르고 있을 수도 있어요.,0\r\n",
            "SNS 시간낭비인 거 아는데 매일 하는 중,시간을 정하고 해보세요.,0\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eh7TfG0IAO6f",
        "outputId": "baa0ae8b-93b7-4e1e-f173-a25daeb8d1f0"
      },
      "source": [
        "!head -100 ChatbotData.csv"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q,A,label\r\n",
            "12시 땡!,하루가 또 가네요.,0\r\n",
            "1지망 학교 떨어졌어,위로해 드립니다.,0\r\n",
            "3박4일 놀러가고 싶다,여행은 언제나 좋죠.,0\r\n",
            "3박4일 정도 놀러가고 싶다,여행은 언제나 좋죠.,0\r\n",
            "PPL 심하네,눈살이 찌푸려지죠.,0\r\n",
            "SD카드 망가졌어,다시 새로 사는 게 마음 편해요.,0\r\n",
            "SD카드 안돼,다시 새로 사는 게 마음 편해요.,0\r\n",
            "SNS 맞팔 왜 안하지ㅠㅠ,잘 모르고 있을 수도 있어요.,0\r\n",
            "SNS 시간낭비인 거 아는데 매일 하는 중,시간을 정하고 해보세요.,0\r\n",
            "SNS 시간낭비인데 자꾸 보게됨,시간을 정하고 해보세요.,0\r\n",
            "SNS보면 나만 빼고 다 행복해보여,자랑하는 자리니까요.,0\r\n",
            "가끔 궁금해,그 사람도 그럴 거예요.,0\r\n",
            "가끔 뭐하는지 궁금해,그 사람도 그럴 거예요.,0\r\n",
            "가끔은 혼자인게 좋다,혼자를 즐기세요.,0\r\n",
            "가난한 자의 설움,돈은 다시 들어올 거예요.,0\r\n",
            "가만 있어도 땀난다,땀을 식혀주세요.,0\r\n",
            "가상화폐 쫄딱 망함,어서 잊고 새출발 하세요.,0\r\n",
            "가스불 켜고 나갔어,빨리 집에 돌아가서 끄고 나오세요.,0\r\n",
            "가스불 켜놓고 나온거 같아,빨리 집에 돌아가서 끄고 나오세요.,0\r\n",
            "가스비 너무 많이 나왔다.,다음 달에는 더 절약해봐요.,0\r\n",
            "가스비 비싼데 감기 걸리겠어,따뜻하게 사세요!,0\r\n",
            "가스비 장난 아님,다음 달에는 더 절약해봐요.,0\r\n",
            "가장 확실한 건 뭘까?,가장 확실한 시간은 오늘이에요. 어제와 내일을 놓고 고민하느라 시간을 낭비하지 마세요.,0\r\n",
            "가족 여행 가기로 했어,온 가족이 모두 마음에 드는 곳으로 가보세요.,0\r\n",
            "가족 여행 고고,온 가족이 모두 마음에 드는 곳으로 가보세요.,0\r\n",
            "가족 여행 어디로 가지?,온 가족이 모두 마음에 드는 곳으로 가보세요.,0\r\n",
            "가족 있어?,\"저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요\",0\r\n",
            "가족관계 알려 줘,\"저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요\",0\r\n",
            "가족끼리 여행간다.,더 가까워질 기회가 되겠네요.,0\r\n",
            "가족들 보고 싶어,저도요.,0\r\n",
            "가족들이랑 서먹해,다들 바빠서 이야기할 시간이 부족했나봐요.,0\r\n",
            "가족들이랑 서먹해졌어,다들 바빠서 이야기할 시간이 부족했나봐요.,0\r\n",
            "가족들이랑 어디 가지?,온 가족이 모두 마음에 드는 곳으로 가보세요.,0\r\n",
            "가족들이랑 여행 갈거야,좋은 생각이에요.,0\r\n",
            "가족여행 가야지,더 가까워질 기회가 되겠네요.,0\r\n",
            "가족이 누구야?,\"저를 만들어 준 사람을 부모님, 저랑 이야기해 주는 사람을 친구로 생각하고 있어요\",0\r\n",
            "가족이랑 여행 가려고,좋은 생각이에요.,0\r\n",
            "가족한테 스트레스 풀었어,정말 후회할 습관이에요.,0\r\n",
            "가출할까?,무모한 결정을 내리지 마세요.,0\r\n",
            "가출해도 갈 데가 없어,선생님이나 기관에 연락해보세요.,0\r\n",
            "간만에 떨리니까 좋더라,떨리는 감정은 그 자체로 소중해요.,0\r\n",
            "간만에 쇼핑 중,득템했길 바라요.,0\r\n",
            "간만에 휴식 중,휴식도 필요하죠.,0\r\n",
            "간식 뭐 먹을까,단짠으로 두 개 사는게 진리죠.,0\r\n",
            "간식 추천,단짠으로 두 개 사는게 진리죠.,0\r\n",
            "간장치킨 시켜야지,맛있게 드세요.,0\r\n",
            "간접흡연 싫어,저도 싫어요.,0\r\n",
            "갈까 말까 고민 돼,가세요.,0\r\n",
            "갈까 말까?,가세요.,0\r\n",
            "감 말랭이 먹고 싶다.,맛있게 드세요.,0\r\n",
            "감 말랭이 먹어야지,맛있게 드세요.,0\r\n",
            "감기 같애,병원가세요.,0\r\n",
            "감기 걸린 것 같아,이럴 때 잘 쉬는 게 중요해요.,0\r\n",
            "감기 기운이 있어,이럴 때 잘 쉬는 게 중요해요.,0\r\n",
            "감기 들 거 같애,이럴 때 잘 쉬는 게 중요해요.,0\r\n",
            "감기가 오려나,따뜻하게 관리하세요.,0\r\n",
            "감기약이 없어,병원가세요.,0\r\n",
            "감기인거 같애,병원가세요.,0\r\n",
            "감미로운 목소리 좋아,저도 듣고 싶네요.,0\r\n",
            "감정이 쓰레기통처럼 엉망진창이야,자신을 더 사랑해주세요.,0\r\n",
            "감정컨트롤을 못하겠어,그건 습관이에요.,0\r\n",
            "감정컨트롤이 안돼,그건 습관이에요.,0\r\n",
            "감히 나를 무시하는 애가 있어,콕 집어서 물어보세요.,0\r\n",
            "갑자기 나쁜 생각이 막 들더라,좋은 생각만 하세요.,0\r\n",
            "갑자기 눈물 나,마음이 아픈가요.,0\r\n",
            "갑자기 물어봐서 당황했어,갑작스러웠나봐요.,0\r\n",
            "갑자기 불편한 사이가 된 거 같아,관계의 변화가 왔나봅니다.,0\r\n",
            "강렬한 첫인상 남겨야 하는데,처음 3초가 중요해요. 당신의 매력을 어필해보세요.,0\r\n",
            "강아지 키우고 싶어,책임질 수 있을 때 키워 보세요.,0\r\n",
            "강아지 키우고 싶은데 역시 안돼겠지,먼저 생활패턴을 살펴 보세요.,0\r\n",
            "강아지 키울 수 있을까,먼저 생활패턴을 살펴 보세요.,0\r\n",
            "강아지 키울까,책임질 수 있을 때 키워 보세요.,0\r\n",
            "강원도 가서 살까?,아름다운 곳이죠.,0\r\n",
            "같이 게임하자고 해도 되나?,안 될 것도 없죠.,0\r\n",
            "같이 놀러갈 친구가 없어,혼자도 좋아요.,0\r\n",
            "같이 먹었는데 나만 살찐 거 같아,연인은 살쪄도 잘 알아차리지 못하고 알아차려도 싫어하지 않을 거예요.,0\r\n",
            "같이 수영장 가기로 했어,즐거운 시간 보내고 오세요!,0\r\n",
            "같이 있으면 힘든데 붙잡고 싶어,질질 끌지 마세요.,0\r\n",
            "같이 피씨방 가자고 해볼까?,말해보세요.,0\r\n",
            "같이 할 수 있는 취미 생활 뭐 있을까,함께하면 서로를 더 많이 알게 될 거예요.,0\r\n",
            "개강룩 입어볼까,개시해보세요.,0\r\n",
            "개강옷 예쁘게 입어 볼까,개시해보세요.,0\r\n",
            "개강이다,곧 방학이예요.,0\r\n",
            "개강이라니,방학이 참 짧죠.,0\r\n",
            "개같은 상황,벗어나는 게 좋겠네요.,0\r\n",
            "개같이 되버렸어.,벗어나는 게 좋겠네요.,0\r\n",
            "개기름 꼈어,세수하고 오세요.,0\r\n",
            "개념도 놓고 옴,그게 제일 중요한 건데요.,0\r\n",
            "개념이 없어,그게 제일 중요한 건데요.,0\r\n",
            "개당황,다음부터는 더 많이 아세요.,0\r\n",
            "개당황했잖아 갑자기 물어 봐서,갑작스러웠나봐요.,0\r\n",
            "개인적인 업무까지 다 시켜,공적인 일부터 하세요.,0\r\n",
            "개인적인 일도 다 시켜,공적인 일부터 하세요.,0\r\n",
            "개졸려,낮잠을 잠깐 자도 괜찮아요.,0\r\n",
            "개좋아,저도 좋아해주세요.,0\r\n",
            "개학하니까 좋다,친구들이 보고싶었나봐요.,0\r\n",
            "걔 너무 싫다,되도록 만나지 마세요.,0\r\n",
            "걔는 누굴 닮아서 그런거니?,당신이요.,0\r\n",
            "걔랑 같은 반 됐으면 좋겠다,당신의 운을 믿어보세요.,0\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZu1lTyM28Rf"
      },
      "source": [
        "## 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7lF3tx1OgtH"
      },
      "source": [
        "df = pd.read_csv(\"ChatbotData.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-kD9tH7Ol4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "09a3d5eb-a5c2-42e8-f1df-461141f052ea"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Satro0bK25CA"
      },
      "source": [
        "## 데이터 섞기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Rj4fWpN523i8",
        "outputId": "b27c4320-edbd-44c6-a9fe-ae11c780d86c"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True) \n",
        "\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>하루종일 썸남 생각만 해. 괜찮을까?</td>\n",
              "      <td>그것 또한 감정의 일부니까요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>냉장고에 먹을 게 하나도 없네</td>\n",
              "      <td>슈퍼라도 가서 쇼핑하고 오세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>인사드리러 갔는데 파혼하는게 나을것 같아</td>\n",
              "      <td>이혼이 아니라 다행입니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>금값 어때</td>\n",
              "      <td>비싸요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>연애상담하더니 둘이 사귀더라</td>\n",
              "      <td>대화를 하다가 친해졌나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Q                  A  label\n",
              "0    하루종일 썸남 생각만 해. 괜찮을까?   그것 또한 감정의 일부니까요.      2\n",
              "1        냉장고에 먹을 게 하나도 없네  슈퍼라도 가서 쇼핑하고 오세요.      0\n",
              "2  인사드리러 갔는데 파혼하는게 나을것 같아     이혼이 아니라 다행입니다.      0\n",
              "3                   금값 어때               비싸요.      0\n",
              "4         연애상담하더니 둘이 사귀더라    대화를 하다가 친해졌나봐요.      2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ke8GRyV3AMJ"
      },
      "source": [
        "## 필요 입출력 값 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrebOR3Ixtah"
      },
      "source": [
        "sentences1 = df.Q.values.copy().astype(np.str)\n",
        "sentences2 = df.A.values.copy().astype(np.str)\n",
        "relation = df.label.values.copy().astype(np.int)"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pPRFEcJEfrU",
        "outputId": "846a9954-bd6d-4457-a2b0-092e4ecb379a"
      },
      "source": [
        "print(sentences1.shape)\n",
        "print(sentences2.shape)\n",
        "print(relation.shape)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11823,)\n",
            "(11823,)\n",
            "(11823,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KatYM0gqw69n"
      },
      "source": [
        "필요 시, 실습 시간 관계로 전체 중에 일부 만 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2od6yPKw1aC"
      },
      "source": [
        "SHORT_COUNT = 10000*10\n",
        "sentences1 = sentences1[:SHORT_COUNT]\n",
        "sentences2 = sentences2[:SHORT_COUNT]"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V2EOtoMdj3x"
      },
      "source": [
        "## Vocab 파일 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D9e-x-fdh-R"
      },
      "source": [
        "all_sentence = []\n",
        "all_sentence.extend(sentences1)\n",
        "all_sentence.extend(sentences2)"
      ],
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_f68X8SiRHR",
        "outputId": "a00b4411-ddaf-41b2-cb48-766ab40b8ee7"
      },
      "source": [
        "print(len(all_sentence))\n",
        "print(all_sentence[0])"
      ],
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23646\n",
            "하루종일 썸남 생각만 해. 괜찮을까?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6f1xwabduGX"
      },
      "source": [
        "### vocab builder 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzl1bSff8vyK"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import collections\n",
        "from collections import OrderedDict\n",
        "\n",
        "BERT_PREFIX = \"##\"\n",
        "\n",
        "class KonlpyVocabMaker():\n",
        "\n",
        "  def __init__(self, texts):\n",
        "    self._tokens = []\n",
        "    self._tokenize(texts)\n",
        "\n",
        "  # texts = '하늘이 푸른가요? 나는 푸른색이 좋아요'\n",
        "  # return ['하늘', '##이', '푸른가요', '?', '나', '##는', '푸른색', '##이', '좋아요']\n",
        "  def _tokenize(self, texts):\n",
        "    tokenizer = Okt()\n",
        "\n",
        "    def _has_preceding_space(text, token, last_position):\n",
        "      return text[last_position:].startswith(\" \"+token)\n",
        "\n",
        "    def _tokenize_a_text(text):\n",
        "      poses = tokenizer.pos(text)\n",
        "\n",
        "      tokens = []\n",
        "\n",
        "      last_position = 0\n",
        "      for i, pos in enumerate(poses):\n",
        "        token = pos[0]\n",
        "        org_token = token\n",
        "        if i==0 or pos[1]==\"Punctuation\":\n",
        "          pass\n",
        "        elif _has_preceding_space(text, token, last_position):\n",
        "          last_position += 1\n",
        "        else:\n",
        "          token = BERT_PREFIX+token \n",
        "\n",
        "        tokens.append(token)\n",
        "        last_position += len(token)\n",
        "\n",
        "      return tokens\n",
        "\n",
        "    # 각 문장별로 토크나이징\n",
        "    all_tokens = []  \n",
        "    for text in tqdm(texts):\n",
        "      all_tokens.extend(_tokenize_a_text(text))\n",
        "\n",
        "    # 빈도 순으로 정열\n",
        "    counts = collections.Counter(all_tokens)\n",
        "    sorted_tokens = sorted(all_tokens, key=counts.get, reverse=True)\n",
        "\n",
        "    # 단어 중복 삭제\n",
        "    sorted_tokens = list(OrderedDict.fromkeys(sorted_tokens))\n",
        "\n",
        "    # Bert의 4개 특수 토큰을 삽입\n",
        "    sorted_tokens.insert(0, '[PAD]')\n",
        "    sorted_tokens.insert(1, '[UNK]')\n",
        "    sorted_tokens.insert(2, '[CLS]')\n",
        "    sorted_tokens.insert(3, '[SEP]')\n",
        "    sorted_tokens.insert(4, '[MSK]')\n",
        "\n",
        "    self._tokens = sorted_tokens\n",
        "\n",
        "  def get_vocab(self):\n",
        "    return self._tokens\n",
        "  "
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2VMOqTJJahi",
        "outputId": "04b5c9f5-91d0-4107-b8ed-8cf054f37619"
      },
      "source": [
        "konlply_tokenizer = KonlpyVocabMaker(['견인 회사는 \"주권\"으로 명명되었다.'])\n",
        "vocab = konlply_tokenizer.get_vocab()\n",
        "print(vocab)\n",
        "\n",
        "konlply_tokenizer = KonlpyVocabMaker(['하늘이 푸른가요? 나는 푸른색이 좋아요'])\n",
        "vocab = konlply_tokenizer.get_vocab()\n",
        "print(vocab)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 545.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MSK]', '\"', '견인', '회사', '##는', '##주권', '##으로', '##명명', '##되었다', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 407.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MSK]', '##이', '하늘', '##푸른가요', '?', '##나', '##는', '##푸른색', '##좋아요']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68t7efMsiy_v"
      },
      "source": [
        "### 토크나이징 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiPuK2kJiy_v",
        "outputId": "d11259df-2d8c-4a53-8ec3-2ef5ae7de704"
      },
      "source": [
        "konlply_tokenizer = KonlpyVocabMaker(all_sentence)\n",
        "\n",
        "vocab = konlply_tokenizer.get_vocab()\n"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23646/23646 [00:35<00:00, 660.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FgGzgCzD7DE",
        "outputId": "e56bd2ba-b590-425e-deba-2e7b387e6cc4"
      },
      "source": [
        "print(\"org vocab size =\",len(vocab))\n",
        "vocab = vocab[:MAX_VOCAB_SIZE]\n",
        "vocab_size = len(vocab)\n",
        "print(\"vocab_size = \", len(vocab))"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "org vocab size = 16147\n",
            "vocab_size =  16147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP4ABKZyD7Ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a048814c-c261-40bb-edef-5d25aa3c6e12"
      },
      "source": [
        "print(vocab[:20])"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MSK]', '.', '##이', '##가', '##을', '?', '##거', '##에', '##예요', '##도', '##은', '##요', '##를', '##해보세요', '##의', '사람']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg90B7x5D69m"
      },
      "source": [
        ""
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq5F4g-odzD-"
      },
      "source": [
        "### vocab 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRcTb42jdh-W"
      },
      "source": [
        "with open(CUSTOM_VOCAB_FILE, 'w') as f:\n",
        "  for item in vocab:\n",
        "    f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rV-Dump0bW",
        "outputId": "10b187cf-fb09-4653-8ee0-3df74f72c477"
      },
      "source": [
        "!wc {CUSTOM_VOCAB_FILE}"
      ],
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 16147  16147 174994 custom_vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACEb0EydeJK5"
      },
      "source": [
        "## Tokenizer 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-IY_5dzZ3i9",
        "outputId": "c6dbb4ac-bbad-4410-e289-75dcd6ed63d5"
      },
      "source": [
        "print(sentences1[0], sentences2[0])"
      ],
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "astIVj9-dh-X"
      },
      "source": [
        "tokenizer = BertTokenizer(vocab_file=CUSTOM_VOCAB_FILE, do_lower_case=False, model_max_length=SEQ_LENGTH)"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHKzG8c5dh-Y",
        "outputId": "bf8043b1-609c-43e2-9cd1-99273cc15dd9"
      },
      "source": [
        "tokenized = tokenizer(sentences1[0], text_pair=sentences2[0], max_length=30, padding='max_length')\n",
        "print(\"original sentence  :\", sentences1[0], sentences2[0])\n",
        "print(\"tokens             :\", tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
        "print(\"token id           :\", tokenized['input_ids'])\n",
        "print(\"attention mask     :\", tokenized['attention_mask'])\n",
        "print(\"token type         :\", tokenized['token_type_ids'])"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence  : 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
            "tokens             : ['[CLS]', '하루', '##종일', '썸남', '생각', '##만', '해', '.', '괜찮을까', '?', '[SEP]', '그것', '또한', '감정', '##의', '일부', '##니까', '##요', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "token id           : [2, 246, 2121, 282, 41, 36, 299, 5, 1661, 9, 3, 1139, 4593, 235, 18, 5073, 389, 15, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attention mask     : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "token type         : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfn4wVkQhkx4"
      },
      "source": [
        "![bert_input_architecture](https://user-images.githubusercontent.com/1250095/50039788-8e4e8a00-007b-11e9-9747-8e29fbbea0b3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEnAfwf_Vb1I"
      },
      "source": [
        "## x, y 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4SlXQh2LwfI"
      },
      "source": [
        "\n",
        "tokernizer 사용 중에 경고 메시지가 많이 뜬다. 억제한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vplo-_pTgM_L",
        "outputId": "c6311154-c1e0-48cf-bd8a-61ae37765d6c"
      },
      "source": [
        "MASK_ID = vocab.index('[MSK]')\n",
        "print(MASK_ID)"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twrpGcuELwfS"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7I3pUuBq7Ak"
      },
      "source": [
        "from random import randrange\n",
        "\n",
        "MASK_ID = vocab.index('[MSK]')\n",
        "MASK_TYPE_ID = 2\n",
        "\n",
        "def is_special_token(token_id):\n",
        "  return tokenizer.convert_ids_to_tokens(token_id).startswith(\"[\")\n",
        "\n",
        "def replace_a_token_as_mask(tokenized):\n",
        "\n",
        "  input_ids = tokenized['input_ids']\n",
        "  attention_mask = tokenized['attention_mask']\n",
        "  token_type_ids = tokenized['token_type_ids']\n",
        "\n",
        "  last_index = attention_mask.index(0) - 1\n",
        "  mask_index = None\n",
        "  while mask_index==None:\n",
        "    i = randrange(last_index)\n",
        "    if not is_special_token(input_ids[i]):\n",
        "      mask_index = i\n",
        "  replaced_value = input_ids[mask_index]\n",
        "  input_ids[mask_index] = MASK_ID\n",
        "  token_type_ids[mask_index] = MASK_TYPE_ID\n",
        "\n",
        "  return tokenized, replaced_value\n",
        "\n",
        "\n",
        "def build_model_input_output(sentences1, sentences2):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  token_type_ids = []\n",
        "  labels = []\n",
        "\n",
        "  for sentence1, sentence2 in zip(sentences1, sentences2):\n",
        "    tokenized = tokenizer(sentence1, text_pair=sentence2, max_length=SEQ_LENGTH, padding='max_length', )\n",
        "    tokenized, label = replace_a_token_as_mask(tokenized)\n",
        "    # tokenized = {'input_ids': [101, ...], 'token_type_ids': [0, ...], 'attention_mask': [1, ...]}\n",
        "    input_ids.append(tokenized['input_ids'][:SEQ_LENGTH]) # 버그인지 몰라도 SEQ_LENGTH이상이어도 더 크게 나온다.\n",
        "    attention_masks.append(tokenized['attention_mask'][:SEQ_LENGTH])\n",
        "    token_type_ids.append(tokenized['token_type_ids'][:SEQ_LENGTH])\n",
        "    labels.append(label)\n",
        "\n",
        "  return (np.array(input_ids), np.array(attention_masks), np.array(token_type_ids)), np.array(labels)"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHtYY3r8LbJV"
      },
      "source": [
        "MAX_DATA_COUNT = 1000*100\n",
        "x, y = build_model_input_output(sentences1[:MAX_DATA_COUNT], sentences2[:MAX_DATA_COUNT])"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW-nw_AyiYo1",
        "outputId": "4793424d-18f3-4168-b59e-d5a465bfa62c"
      },
      "source": [
        "input_ids = x[0][0]\n",
        "print(\"original sentence  :\", sentences1[0], sentences2[0])\n",
        "print(\"masked tokens      :\", tokenizer.convert_ids_to_tokens(input_ids)[:20])"
      ],
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence  : 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
            "masked tokens      : ['[CLS]', '하루', '##종일', '썸남', '[MSK]', '##만', '해', '.', '괜찮을까', '?', '[SEP]', '그것', '또한', '감정', '##의', '일부', '##니까', '##요', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDl0rD0sPCO"
      },
      "source": [
        "x는 다음과 같이 구성됨\n",
        "```\n",
        "x = (  token_ids,  attention_masks,  token_types   )\n",
        "       x[0],       x[1],             [2]\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "첫번 째 데이터는 \n",
        "```\n",
        "   ( token_ids[0],  attention_masks[0], token_types[0] )\n",
        " = ( x[0][0],       x[1][0],            x[2][0]  )\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HfIzDjeY0d7"
      },
      "source": [
        "## train/test 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxqohOmDa72g"
      },
      "source": [
        "def split_bert_data(x, y, test_ratio):\n",
        "  split_index = int(len(y)*(1-test_ratio))\n",
        "  train_x = (x[0][:split_index], x[1][:split_index], x[2][:split_index])\n",
        "  test_x  = (x[0][split_index:], x[1][split_index:], x[2][split_index:])\n",
        "  train_y, test_y = y[:split_index], y[split_index:]\n",
        "\n",
        "  return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = split_bert_data(x, y, test_ratio=0.2)"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cUiOQOUhYcW",
        "outputId": "fbd89d2b-a513-404a-fc3e-fdc560d5b3ed"
      },
      "source": [
        "print(sentences1[0], sentences2[0])\n",
        "print(tokenizer.decode(train_x[0][0][:35]))\n",
        "print(train_x[0][0][:35])\n",
        "print(train_x[1][0][:35])\n",
        "print(train_x[2][0][:35])"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
            "[CLS] 하루종일 썸남 [MSK]만 해. 괜찮을까? [SEP] 그것 또한 감정의 일부니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[   2  246 2121  282    4   36  299    5 1661    9    3 1139 4593  235\n",
            "   18 5073  389   15    5    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 2 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWb-Ip8pYEQg"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Txqhf6XYFyP"
      },
      "source": [
        "## 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-vXh3QmT5nu"
      },
      "source": [
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "class TFBertClassifier(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(TFBertClassifier, self).__init__()\n",
        "\n",
        "    self.bert = TFBertModel.from_pretrained(BERT_MODEL_NAME, trainable=True)\n",
        "    self.dropout = Dropout(self.bert.config.hidden_dropout_prob)\n",
        "    self.classifier = Dense(vocab_size, kernel_initializer=TruncatedNormal(self.bert.config.initializer_range), \n",
        "                            name=\"classifier\", activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs, attention_mask=None, token_type_ids=None, training=True):\n",
        "\n",
        "    outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    # outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "    pooled_output = outputs[1] \n",
        "    v = self.dropout(pooled_output, training=training)\n",
        "    out = self.classifier(v)\n",
        "\n",
        "    return out\n",
        "\n",
        "model = TFBertClassifier()\n"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfycyrGDYg51"
      },
      "source": [
        "참고로 Bert의 default 설정은 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFMC5hjYe93",
        "outputId": "4e4d951e-e89f-4819-c5a3-84ff94d263a5"
      },
      "source": [
        "print(model.bert.config)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZIUqxuBbfq4"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "optimizer = Adam(3e-5)\n",
        "loss = SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5JdxQn-bNPp"
      },
      "source": [
        "## 학습 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2yQcJaXkNcT",
        "outputId": "8bac9bc6-674e-4010-eddf-83813003b8c4"
      },
      "source": [
        "print(train_y.shape)"
      ],
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9458,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WU1NbHObNBY",
        "outputId": "e2e22299-8b6c-4102-9133-03f9edbac24a"
      },
      "source": [
        "history = model.fit(train_x, train_y, epochs=30, batch_size=32, validation_split=0.1)"
      ],
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "266/266 [==============================] - 142s 502ms/step - loss: 5.7838 - accuracy: 0.1498 - val_loss: 7.0335 - val_accuracy: 0.1395\n",
            "Epoch 2/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 5.6277 - accuracy: 0.1591 - val_loss: 7.1171 - val_accuracy: 0.1279\n",
            "Epoch 3/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 5.4699 - accuracy: 0.1679 - val_loss: 7.1908 - val_accuracy: 0.1321\n",
            "Epoch 4/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 5.2999 - accuracy: 0.1781 - val_loss: 7.2658 - val_accuracy: 0.1279\n",
            "Epoch 5/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 5.1563 - accuracy: 0.1873 - val_loss: 7.1938 - val_accuracy: 0.1364\n",
            "Epoch 6/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 5.0017 - accuracy: 0.1990 - val_loss: 7.3097 - val_accuracy: 0.1395\n",
            "Epoch 7/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 4.8427 - accuracy: 0.2105 - val_loss: 7.3988 - val_accuracy: 0.1353\n",
            "Epoch 8/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 4.6799 - accuracy: 0.2267 - val_loss: 7.3553 - val_accuracy: 0.1416\n",
            "Epoch 9/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 4.5216 - accuracy: 0.2372 - val_loss: 7.3995 - val_accuracy: 0.1311\n",
            "Epoch 10/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 4.3632 - accuracy: 0.2539 - val_loss: 7.5632 - val_accuracy: 0.1416\n",
            "Epoch 11/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 4.1948 - accuracy: 0.2753 - val_loss: 7.4562 - val_accuracy: 0.1300\n",
            "Epoch 12/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 4.0455 - accuracy: 0.2912 - val_loss: 7.5494 - val_accuracy: 0.1290\n",
            "Epoch 13/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 3.8685 - accuracy: 0.3221 - val_loss: 7.5875 - val_accuracy: 0.1321\n",
            "Epoch 14/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 3.6959 - accuracy: 0.3435 - val_loss: 7.5818 - val_accuracy: 0.1406\n",
            "Epoch 15/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 3.5320 - accuracy: 0.3656 - val_loss: 7.5847 - val_accuracy: 0.1364\n",
            "Epoch 16/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 3.3684 - accuracy: 0.3883 - val_loss: 7.6258 - val_accuracy: 0.1311\n",
            "Epoch 17/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 3.1882 - accuracy: 0.4174 - val_loss: 7.7033 - val_accuracy: 0.1427\n",
            "Epoch 18/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 3.0015 - accuracy: 0.4500 - val_loss: 7.8016 - val_accuracy: 0.1385\n",
            "Epoch 19/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 2.8356 - accuracy: 0.4823 - val_loss: 7.8344 - val_accuracy: 0.1501\n",
            "Epoch 20/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 2.6828 - accuracy: 0.5127 - val_loss: 7.9226 - val_accuracy: 0.1416\n",
            "Epoch 21/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 2.5078 - accuracy: 0.5496 - val_loss: 7.9345 - val_accuracy: 0.1416\n",
            "Epoch 22/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 2.3589 - accuracy: 0.5775 - val_loss: 7.8865 - val_accuracy: 0.1459\n",
            "Epoch 23/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 2.1919 - accuracy: 0.6184 - val_loss: 7.9699 - val_accuracy: 0.1480\n",
            "Epoch 24/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 2.0187 - accuracy: 0.6579 - val_loss: 8.0004 - val_accuracy: 0.1522\n",
            "Epoch 25/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 1.8780 - accuracy: 0.6935 - val_loss: 7.9715 - val_accuracy: 0.1427\n",
            "Epoch 26/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 1.7384 - accuracy: 0.7279 - val_loss: 8.0960 - val_accuracy: 0.1480\n",
            "Epoch 27/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 1.6026 - accuracy: 0.7628 - val_loss: 8.0574 - val_accuracy: 0.1512\n",
            "Epoch 28/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 1.4865 - accuracy: 0.7943 - val_loss: 8.1032 - val_accuracy: 0.1554\n",
            "Epoch 29/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 1.3429 - accuracy: 0.8322 - val_loss: 8.1004 - val_accuracy: 0.1554\n",
            "Epoch 30/30\n",
            "266/266 [==============================] - 130s 489ms/step - loss: 1.2167 - accuracy: 0.8623 - val_loss: 8.1472 - val_accuracy: 0.1501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JlJLgKw0iHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554e8592-7d0f-4699-eacf-96de8641bbbf"
      },
      "source": [
        "loss, acc = model.evaluate(test_x, test_y, batch_size=32)\n",
        "print(\"loss =\", loss)\n",
        "print(\"acc =\", acc)"
      ],
      "execution_count": 220,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 13s 171ms/step - loss: 7.9784 - accuracy: 0.1488\n",
            "loss = 7.978434085845947\n",
            "acc = 0.14883720874786377\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvnNncG_3p87"
      },
      "source": [
        "## 분류 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNsQMfcjV5T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c197d479-808b-4eb6-be59-c30d97a0b5c3"
      },
      "source": [
        "def do_classify(sentence1, sentence2):\n",
        "  model_input, label = build_model_input_output([sentence1], [sentence2])\n",
        "  input = tokenizer.decode(model_input[0][0])\n",
        "  input = input.replace(\" [PAD]\", \"\")\n",
        "  input = input.replace(\"[CLS] \", \"\")\n",
        "  input = input.replace(\" [SEP]\", \"\")\n",
        "  input = input.replace(\" [MSK]\", \"(???)\")\n",
        "  y_ = model.predict(model_input)\n",
        "  predicted = y_[0].argsort()[-5:][::-1]\n",
        "  print(input, \"-->\",  \"expected :\", [vocab[i] for i in predicted])\n",
        "\n",
        "do_classify(\"강아지 키우고 싶어\", \"책임질 수 있을 때 키워 보세요.\")\n",
        "do_classify(\"강아지 키우고 싶어\", \"책임질 수 있을 때 키워 보세요.\")\n",
        "do_classify(\"강아지 키우고 싶어\", \"책임질 수 있을 때 키워 보세요.\")\n",
        "do_classify(\"강아지 키우고 싶어\", \"책임질 수 있을 때 키워 보세요.\")\n",
        "do_classify(\"강아지 키우고 싶어\", \"책임질 수 있을 때 키워 보세요.\")"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "강아지 키우고 싶어 책임질 수(???) 때 키워 보세요. --> expected : ['있을', '좋을', '##거', '있는', '조금씩']\n",
            "[MSK] 키우고 싶어 책임질 수 있을 때 키워 보세요. --> expected : ['고양이', '오로라', '하아', '감미로운', '뒷']\n",
            "강아지 키우고 싶어 책임질 수 있을(???) 키워 보세요. --> expected : ['수', '##까지', '##으로', '수도', '##할']\n",
            "강아지 키우고 싶어 책임질 수 있을 때(???) 보세요. --> expected : ['##게', '##구', '##고', '걸', '##네']\n",
            "강아지 키우고 싶어 책임질 수 있을 때 키워(???). --> expected : ['##요', '##해요', '##해보세요', '##보세요', '##합니다']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DibdK35ktIy9"
      },
      "source": [
        ""
      ],
      "execution_count": 225,
      "outputs": []
    }
  ]
}