{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "korean_mask_completion_with_bert_with_custom_vocab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pyz8OyCRKRz8"
      },
      "source": [
        "# Bert를 사용한 괄호 단어 찾기. 사용자 vocab을 사용한\n",
        "\n",
        "2개의 채팅 문답에서 한 부분을 괄호 치고, 이 괄호 친 부분의 단어를 예측.\n",
        "\n",
        "```\n",
        " 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
        "\n",
        " 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 (??)의 일부니까요.\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBC-mz4q4kIr"
      },
      "source": [
        "copy from https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/blob/master/7.PRETRAIN_METHOD/7.2.2.bert_finetune_KorNLI.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFwbg7k1KZrK"
      },
      "source": [
        "# 필요 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTUt2OoT04Kh",
        "outputId": "50860bc2-e936-404a-8c6c-b1ab861b355c"
      },
      "source": [
        "!pip install transformers==3.0.2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==3.0.2\n",
            "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
            "\u001b[?25l\r\u001b[K     |▍                               | 10 kB 36.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 20 kB 21.2 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40 kB 14.3 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 71 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 143 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 194 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 769 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "  Downloading tokenizers-0.8.1rc1-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 58.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.46 sentencepiece-0.1.96 tokenizers-0.8.1rc1 transformers-3.0.2\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (0.1.96)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVofHG0eh40t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb45358-2553-4f57-e7f5-296db3a90155"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, konlpy\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOH1dbiHh5_S"
      },
      "source": [
        "# 셋업"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-FHX9tOK4pV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "from transformers import TFBertModel\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex8GjpDqK7Lm"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "SEQ_LENGTH = 128\n",
        "BERT_MODEL_NAME = 'bert-base-multilingual-cased'\n",
        "CUSTOM_VOCAB_FILE = 'custom_vocab.txt'\n",
        "MAX_VOCAB_SIZE = 10000*2"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "104nTDhQLQmP"
      },
      "source": [
        "# 데이터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bobHiFxmBN01"
      },
      "source": [
        "## 데이터 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjJehKcMOA9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ee90b2-8115-42f0-ec01-bf2635c0986c"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-21 02:00:19--  https://raw.githubusercontent.com/songys/Chatbot_data/master/ChatbotData.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 889842 (869K) [text/plain]\n",
            "Saving to: ‘ChatbotData.csv’\n",
            "\n",
            "ChatbotData.csv     100%[===================>] 868.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2021-11-21 02:00:20 (28.3 MB/s) - ‘ChatbotData.csv’ saved [889842/889842]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7ILWTRSQBWT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4a478c-319f-4386-c5e0-309d01de5d07"
      },
      "source": [
        "!head ChatbotData.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q,A,label\r\n",
            "12시 땡!,하루가 또 가네요.,0\r\n",
            "1지망 학교 떨어졌어,위로해 드립니다.,0\r\n",
            "3박4일 놀러가고 싶다,여행은 언제나 좋죠.,0\r\n",
            "3박4일 정도 놀러가고 싶다,여행은 언제나 좋죠.,0\r\n",
            "PPL 심하네,눈살이 찌푸려지죠.,0\r\n",
            "SD카드 망가졌어,다시 새로 사는 게 마음 편해요.,0\r\n",
            "SD카드 안돼,다시 새로 사는 게 마음 편해요.,0\r\n",
            "SNS 맞팔 왜 안하지ㅠㅠ,잘 모르고 있을 수도 있어요.,0\r\n",
            "SNS 시간낭비인 거 아는데 매일 하는 중,시간을 정하고 해보세요.,0\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZu1lTyM28Rf"
      },
      "source": [
        "## 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7lF3tx1OgtH"
      },
      "source": [
        "df = pd.read_csv(\"ChatbotData.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-kD9tH7Ol4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "265591f2-d381-4b0e-b278-bf0b591d3d19"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12시 땡!</td>\n",
              "      <td>하루가 또 가네요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1지망 학교 떨어졌어</td>\n",
              "      <td>위로해 드립니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3박4일 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3박4일 정도 놀러가고 싶다</td>\n",
              "      <td>여행은 언제나 좋죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>PPL 심하네</td>\n",
              "      <td>눈살이 찌푸려지죠.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 Q            A  label\n",
              "0           12시 땡!   하루가 또 가네요.      0\n",
              "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
              "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
              "4          PPL 심하네   눈살이 찌푸려지죠.      0"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Satro0bK25CA"
      },
      "source": [
        "## 데이터 섞기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Rj4fWpN523i8",
        "outputId": "df37f177-d9fa-4d16-ec16-3be0fff58b88"
      },
      "source": [
        "df = df.sample(frac=1).reset_index(drop=True) \n",
        "\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Q</th>\n",
              "      <th>A</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>하루종일 썸남 생각만 해. 괜찮을까?</td>\n",
              "      <td>그것 또한 감정의 일부니까요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>냉장고에 먹을 게 하나도 없네</td>\n",
              "      <td>슈퍼라도 가서 쇼핑하고 오세요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>인사드리러 갔는데 파혼하는게 나을것 같아</td>\n",
              "      <td>이혼이 아니라 다행입니다.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>금값 어때</td>\n",
              "      <td>비싸요.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>연애상담하더니 둘이 사귀더라</td>\n",
              "      <td>대화를 하다가 친해졌나봐요.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Q                  A  label\n",
              "0    하루종일 썸남 생각만 해. 괜찮을까?   그것 또한 감정의 일부니까요.      2\n",
              "1        냉장고에 먹을 게 하나도 없네  슈퍼라도 가서 쇼핑하고 오세요.      0\n",
              "2  인사드리러 갔는데 파혼하는게 나을것 같아     이혼이 아니라 다행입니다.      0\n",
              "3                   금값 어때               비싸요.      0\n",
              "4         연애상담하더니 둘이 사귀더라    대화를 하다가 친해졌나봐요.      2"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ke8GRyV3AMJ"
      },
      "source": [
        "## 필요 입출력 값 준비"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrebOR3Ixtah"
      },
      "source": [
        "sentences1 = df.Q.values.copy().astype(np.str)\n",
        "sentences2 = df.A.values.copy().astype(np.str)\n",
        "relation = df.label.values.copy().astype(np.int)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pPRFEcJEfrU",
        "outputId": "3f998dd8-7bcc-44fd-a2f4-26229d335ee3"
      },
      "source": [
        "print(sentences1.shape)\n",
        "print(sentences2.shape)\n",
        "print(relation.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11823,)\n",
            "(11823,)\n",
            "(11823,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KatYM0gqw69n"
      },
      "source": [
        "필요 시, 실습 시간 관계로 전체 중에 일부 만 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2od6yPKw1aC"
      },
      "source": [
        "SHORT_COUNT = 10000*10\n",
        "sentences1 = sentences1[:SHORT_COUNT]\n",
        "sentences2 = sentences2[:SHORT_COUNT]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V2EOtoMdj3x"
      },
      "source": [
        "## Vocab 파일 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_D9e-x-fdh-R"
      },
      "source": [
        "all_sentence = []\n",
        "all_sentence.extend(sentences1)\n",
        "all_sentence.extend(sentences2)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_f68X8SiRHR",
        "outputId": "fec03535-1c6b-4380-9d4f-5b87b575a2c5"
      },
      "source": [
        "print(len(all_sentence))\n",
        "print(all_sentence[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23646\n",
            "하루종일 썸남 생각만 해. 괜찮을까?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6f1xwabduGX"
      },
      "source": [
        "### vocab builder 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzl1bSff8vyK"
      },
      "source": [
        "from konlpy.tag import Okt\n",
        "import collections\n",
        "from collections import OrderedDict\n",
        "\n",
        "BERT_PREFIX = \"##\"\n",
        "\n",
        "class KonlpyVocabMaker():\n",
        "\n",
        "  def __init__(self, texts):\n",
        "    self._tokens = []\n",
        "    self._tokenize(texts)\n",
        "\n",
        "  # texts = '하늘이 푸른가요? 나는 푸른색이 좋아요'\n",
        "  # return ['하늘', '##이', '푸른가요', '?', '나', '##는', '푸른색', '##이', '좋아요']\n",
        "  def _tokenize(self, texts):\n",
        "    tokenizer = Okt()\n",
        "\n",
        "    def _has_preceding_space(text, token, last_position):\n",
        "      return text[last_position:].startswith(\" \"+token)\n",
        "\n",
        "    def _tokenize_a_text(text):\n",
        "      poses = tokenizer.pos(text)\n",
        "\n",
        "      tokens = []\n",
        "\n",
        "      last_position = 0\n",
        "      for i, pos in enumerate(poses):\n",
        "        org_token = pos[0]\n",
        "        token = pos[0]\n",
        "        if i==0:\n",
        "          pass\n",
        "        elif pos[1]==\"Punctuation\":\n",
        "          if _has_preceding_space(text, token, last_position):  # \" 'of\"\n",
        "            last_position += 1\n",
        "          else:                                                 # \"습니다.\"\n",
        "            pass\n",
        "        elif _has_preceding_space(text, token, last_position):\n",
        "          last_position += 1\n",
        "        elif pos[1]==\"Alpha\":\n",
        "          pass\n",
        "        else:\n",
        "          token = BERT_PREFIX+token \n",
        "\n",
        "        tokens.append(token)\n",
        "        last_position += len(org_token)\n",
        "\n",
        "      return tokens\n",
        "\n",
        "    # 각 문장별로 토크나이징\n",
        "    all_tokens = []  \n",
        "    for text in tqdm(texts):\n",
        "      all_tokens.extend(_tokenize_a_text(text))\n",
        "\n",
        "    # 빈도 순으로 정열\n",
        "    counts = collections.Counter(all_tokens)\n",
        "    sorted_tokens = sorted(all_tokens, key=counts.get, reverse=True)\n",
        "\n",
        "    # 단어 중복 삭제\n",
        "    sorted_tokens = list(OrderedDict.fromkeys(sorted_tokens))\n",
        "\n",
        "    # Bert의 4개 특수 토큰을 삽입\n",
        "    sorted_tokens.insert(0, '[PAD]')\n",
        "    sorted_tokens.insert(1, '[UNK]')\n",
        "    sorted_tokens.insert(2, '[CLS]')\n",
        "    sorted_tokens.insert(3, '[SEP]')\n",
        "    sorted_tokens.insert(4, '[MSK]')\n",
        "\n",
        "    self._tokens = sorted_tokens\n",
        "\n",
        "  def get_vocab(self):\n",
        "    return self._tokens\n",
        "  "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2VMOqTJJahi",
        "outputId": "71885440-a62e-41ff-ccf1-023f614a924e"
      },
      "source": [
        "konlply_tokenizer = KonlpyVocabMaker(['견인 회사는 \"주권\"으로 명명되었다.'])\n",
        "vocab = konlply_tokenizer.get_vocab()\n",
        "print(vocab)\n",
        "\n",
        "konlply_tokenizer = KonlpyVocabMaker(['하늘이 푸른가요? 나는 푸른색이 좋아요'])\n",
        "vocab = konlply_tokenizer.get_vocab()\n",
        "print(vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:06<00:00,  6.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MSK]', '\"', '견인', '회사', '##는', '##주권', '##으로', '##명명', '##되었다', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 26.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MSK]', '##이', '하늘', '푸른가요', '?', '나', '##는', '푸른색', '좋아요']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68t7efMsiy_v"
      },
      "source": [
        "### 토크나이징 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiPuK2kJiy_v",
        "outputId": "688b39ac-01e5-479e-b682-3192cc78b518"
      },
      "source": [
        "konlply_tokenizer = KonlpyVocabMaker(all_sentence)\n",
        "\n",
        "vocab = konlply_tokenizer.get_vocab()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23646/23646 [00:24<00:00, 947.82it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FgGzgCzD7DE",
        "outputId": "3d706627-8fee-40e4-967f-5fb81aef86ec"
      },
      "source": [
        "print(\"org vocab size =\",len(vocab))\n",
        "vocab = vocab[:MAX_VOCAB_SIZE]\n",
        "vocab_size = len(vocab)\n",
        "print(\"vocab_size = \", len(vocab))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "org vocab size = 13974\n",
            "vocab_size =  13974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP4ABKZyD7Ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164d93b3-c80b-470d-f201-7a29a6b3b551"
      },
      "source": [
        "print(vocab[:20])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MSK]', '.', '##이', '##가', '##을', '?', '거', '사람', '##에', '##예요', '##은', '##도', '##요', '##를', '사랑', '생각']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg90B7x5D69m"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pq5F4g-odzD-"
      },
      "source": [
        "### vocab 파일 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRcTb42jdh-W"
      },
      "source": [
        "with open(CUSTOM_VOCAB_FILE, 'w') as f:\n",
        "  for item in vocab:\n",
        "    f.write(\"%s\\n\" % item)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9rV-Dump0bW",
        "outputId": "360e901e-c91b-472d-899e-461654778a77"
      },
      "source": [
        "!wc {CUSTOM_VOCAB_FILE}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 13974  13974 142422 custom_vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACEb0EydeJK5"
      },
      "source": [
        "## Tokenizer 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-IY_5dzZ3i9",
        "outputId": "4296a66a-e013-42e3-94db-234d6298fcff"
      },
      "source": [
        "print(sentences1[0], sentences2[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "astIVj9-dh-X"
      },
      "source": [
        "tokenizer = BertTokenizer(vocab_file=CUSTOM_VOCAB_FILE, do_lower_case=False, model_max_length=SEQ_LENGTH)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHKzG8c5dh-Y",
        "outputId": "512256ff-c7b4-4f87-e911-4c715bad8d71"
      },
      "source": [
        "tokenized = tokenizer(sentences1[0], text_pair=sentences2[0], max_length=30, padding='max_length')\n",
        "print(\"original sentence  :\", sentences1[0], sentences2[0])\n",
        "print(\"tokens             :\", tokenizer.convert_ids_to_tokens(tokenized['input_ids']))\n",
        "print(\"token id           :\", tokenized['input_ids'])\n",
        "print(\"attention mask     :\", tokenized['attention_mask'])\n",
        "print(\"token type         :\", tokenized['token_type_ids'])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence  : 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
            "tokens             : ['[CLS]', '하루', '##종일', '썸남', '생각', '##만', '해', '.', '괜찮을까', '?', '[SEP]', '그것', '또한', '감정', '##의', '일부', '##니까', '##요', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "token id           : [2, 201, 2202, 255, 19, 42, 84, 5, 630, 9, 3, 1052, 3281, 174, 22, 2890, 387, 16, 5, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "attention mask     : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "token type         : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfn4wVkQhkx4"
      },
      "source": [
        "![bert_input_architecture](https://user-images.githubusercontent.com/1250095/50039788-8e4e8a00-007b-11e9-9747-8e29fbbea0b3.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEnAfwf_Vb1I"
      },
      "source": [
        "## x, y 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4SlXQh2LwfI"
      },
      "source": [
        "\n",
        "tokernizer 사용 중에 경고 메시지가 많이 뜬다. 억제한다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vplo-_pTgM_L",
        "outputId": "2ffeabe6-83ca-4044-c476-b9eb8efb6fe6"
      },
      "source": [
        "MASK_ID = vocab.index('[MSK]')\n",
        "print(MASK_ID)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twrpGcuELwfS"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7I3pUuBq7Ak"
      },
      "source": [
        "from random import randrange\n",
        "\n",
        "MASK_ID = vocab.index('[MSK]')\n",
        "MASK_TYPE_ID = 2\n",
        "\n",
        "def is_special_token(token_id):\n",
        "  return tokenizer.convert_ids_to_tokens(token_id).startswith(\"[\")\n",
        "\n",
        "def replace_a_token_as_mask(tokenized):\n",
        "\n",
        "  input_ids = tokenized['input_ids']\n",
        "  attention_mask = tokenized['attention_mask']\n",
        "  token_type_ids = tokenized['token_type_ids']\n",
        "\n",
        "  last_index = attention_mask.index(0) - 1\n",
        "  mask_index = None\n",
        "  while mask_index==None:\n",
        "    i = randrange(last_index)\n",
        "    if not is_special_token(input_ids[i]):\n",
        "      mask_index = i\n",
        "  replaced_value = input_ids[mask_index]\n",
        "  input_ids[mask_index] = MASK_ID\n",
        "  token_type_ids[mask_index] = MASK_TYPE_ID\n",
        "\n",
        "  return tokenized, replaced_value\n",
        "\n",
        "\n",
        "def build_model_input_output(sentences1, sentences2):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  token_type_ids = []\n",
        "  labels = []\n",
        "\n",
        "  for sentence1, sentence2 in zip(sentences1, sentences2):\n",
        "    tokenized = tokenizer(sentence1, text_pair=sentence2, max_length=SEQ_LENGTH, padding='max_length', )\n",
        "    tokenized, label = replace_a_token_as_mask(tokenized)\n",
        "    # tokenized = {'input_ids': [101, ...], 'token_type_ids': [0, ...], 'attention_mask': [1, ...]}\n",
        "    input_ids.append(tokenized['input_ids'][:SEQ_LENGTH]) # 버그인지 몰라도 SEQ_LENGTH이상이어도 더 크게 나온다.\n",
        "    attention_masks.append(tokenized['attention_mask'][:SEQ_LENGTH])\n",
        "    token_type_ids.append(tokenized['token_type_ids'][:SEQ_LENGTH])\n",
        "    labels.append(label)\n",
        "\n",
        "  return (np.array(input_ids), np.array(attention_masks), np.array(token_type_ids)), np.array(labels)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHtYY3r8LbJV"
      },
      "source": [
        "MAX_DATA_COUNT = 1000*100\n",
        "x, y = build_model_input_output(sentences1[:MAX_DATA_COUNT], sentences2[:MAX_DATA_COUNT])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iW-nw_AyiYo1",
        "outputId": "0af414c6-25e5-4737-ae42-d7918bc0dbe8"
      },
      "source": [
        "input_ids = x[0][0]\n",
        "print(\"original sentence  :\", sentences1[0], sentences2[0])\n",
        "print(\"masked tokens      :\", tokenizer.convert_ids_to_tokens(input_ids)[:20])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original sentence  : 하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
            "masked tokens      : ['[CLS]', '하루', '##종일', '썸남', '생각', '##만', '[MSK]', '.', '괜찮을까', '?', '[SEP]', '그것', '또한', '감정', '##의', '일부', '##니까', '##요', '.', '[SEP]']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adDl0rD0sPCO"
      },
      "source": [
        "x는 다음과 같이 구성됨\n",
        "```\n",
        "x = (  token_ids,  attention_masks,  token_types   )\n",
        "       x[0],       x[1],             [2]\n",
        "```\n",
        "\n",
        "<br>\n",
        "\n",
        "첫번 째 데이터는 \n",
        "```\n",
        "   ( token_ids[0],  attention_masks[0], token_types[0] )\n",
        " = ( x[0][0],       x[1][0],            x[2][0]  )\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HfIzDjeY0d7"
      },
      "source": [
        "## train/test 분리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxqohOmDa72g"
      },
      "source": [
        "def split_bert_data(x, y, test_ratio):\n",
        "  split_index = int(len(y)*(1-test_ratio))\n",
        "  train_x = (x[0][:split_index], x[1][:split_index], x[2][:split_index])\n",
        "  test_x  = (x[0][split_index:], x[1][split_index:], x[2][split_index:])\n",
        "  train_y, test_y = y[:split_index], y[split_index:]\n",
        "\n",
        "  return (train_x, train_y), (test_x, test_y)\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = split_bert_data(x, y, test_ratio=0.2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cUiOQOUhYcW",
        "outputId": "925daedc-d3cf-456b-8946-baafd5a05d59"
      },
      "source": [
        "print(sentences1[0], sentences2[0])\n",
        "print(tokenizer.decode(train_x[0][0][:35]))\n",
        "print(train_x[0][0][:35])\n",
        "print(train_x[1][0][:35])\n",
        "print(train_x[2][0][:35])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "하루종일 썸남 생각만 해. 괜찮을까? 그것 또한 감정의 일부니까요.\n",
            "[CLS] 하루종일 썸남 생각만 [MSK]. 괜찮을까? [SEP] 그것 또한 감정의 일부니까요. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "[   2  201 2202  255   19   42    4    5  630    9    3 1052 3281  174\n",
            "   22 2890  387   16    5    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0]\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 2 0 0 0 0 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWb-Ip8pYEQg"
      },
      "source": [
        "# 학습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Txqhf6XYFyP"
      },
      "source": [
        "## 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-vXh3QmT5nu"
      },
      "source": [
        "from tensorflow.keras.initializers import TruncatedNormal\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "class TFBertClassifier(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(TFBertClassifier, self).__init__()\n",
        "\n",
        "    self.bert = TFBertModel.from_pretrained(BERT_MODEL_NAME, trainable=True)\n",
        "    self.dropout = Dropout(self.bert.config.hidden_dropout_prob)\n",
        "    self.classifier = Dense(vocab_size, kernel_initializer=TruncatedNormal(self.bert.config.initializer_range), \n",
        "                            name=\"classifier\", activation=\"softmax\")\n",
        "\n",
        "  def call(self, inputs, attention_mask=None, token_type_ids=None, training=True):\n",
        "\n",
        "    outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "    # outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "    pooled_output = outputs[1] \n",
        "    v = self.dropout(pooled_output, training=training)\n",
        "    out = self.classifier(v)\n",
        "\n",
        "    return out\n",
        "\n",
        "model = TFBertClassifier()\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfycyrGDYg51"
      },
      "source": [
        "참고로 Bert의 default 설정은 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6rFMC5hjYe93",
        "outputId": "5208bfe7-efc5-40ec-a8bf-ba4282ac48b3"
      },
      "source": [
        "print(model.bert.config)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZIUqxuBbfq4"
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "optimizer = Adam(3e-5)\n",
        "loss = SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5JdxQn-bNPp"
      },
      "source": [
        "## 학습 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2yQcJaXkNcT",
        "outputId": "a92c94c5-c18a-4763-b3d3-017009873a2e"
      },
      "source": [
        "print(train_y.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9458,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WU1NbHObNBY",
        "outputId": "dda3d8a3-ff8c-43cd-db62-108c8e972658"
      },
      "source": [
        "history = model.fit(train_x, train_y, epochs=10, batch_size=32, validation_split=0.1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "266/266 [==============================] - 145s 503ms/step - loss: 8.0608 - accuracy: 0.1097 - val_loss: 7.2592 - val_accuracy: 0.1152\n",
            "Epoch 2/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 6.7596 - accuracy: 0.1143 - val_loss: 7.2104 - val_accuracy: 0.1152\n",
            "Epoch 3/10\n",
            "266/266 [==============================] - 130s 487ms/step - loss: 6.5498 - accuracy: 0.1171 - val_loss: 7.2167 - val_accuracy: 0.1300\n",
            "Epoch 4/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 6.4192 - accuracy: 0.1229 - val_loss: 7.1855 - val_accuracy: 0.1332\n",
            "Epoch 5/10\n",
            "266/266 [==============================] - 129s 486ms/step - loss: 6.2942 - accuracy: 0.1340 - val_loss: 7.2280 - val_accuracy: 0.1342\n",
            "Epoch 6/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 6.1865 - accuracy: 0.1414 - val_loss: 7.1968 - val_accuracy: 0.1406\n",
            "Epoch 7/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 6.0773 - accuracy: 0.1446 - val_loss: 7.1805 - val_accuracy: 0.1406\n",
            "Epoch 8/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 5.9477 - accuracy: 0.1521 - val_loss: 7.1316 - val_accuracy: 0.1448\n",
            "Epoch 9/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 5.8067 - accuracy: 0.1543 - val_loss: 7.1697 - val_accuracy: 0.1543\n",
            "Epoch 10/10\n",
            "266/266 [==============================] - 129s 487ms/step - loss: 5.6459 - accuracy: 0.1613 - val_loss: 7.1623 - val_accuracy: 0.1364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JlJLgKw0iHT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88e407f4-f6ae-430f-c1d5-fe909eb20cac"
      },
      "source": [
        "loss, acc = model.evaluate(test_x, test_y, batch_size=32)\n",
        "print(\"loss =\", loss)\n",
        "print(\"acc =\", acc)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74/74 [==============================] - 13s 169ms/step - loss: 7.2420 - accuracy: 0.1349\n",
            "loss = 7.24198579788208\n",
            "acc = 0.13488371670246124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvnNncG_3p87"
      },
      "source": [
        "## 분류 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNsQMfcjV5T8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eed1bbb-5f5f-4850-8ec7-802730cb4c94"
      },
      "source": [
        "def do_classify(sentence1, sentence2):\n",
        "  model_input, label = build_model_input_output([sentence1], [sentence2])\n",
        "  input = tokenizer.decode(model_input[0][0])\n",
        "  input = input.replace(\" [PAD]\", \"\")\n",
        "  input = input.replace(\"[CLS] \", \"\")\n",
        "  input = input.replace(\" [SEP]\", \"\")\n",
        "  input = input.replace(\" [MSK]\", \"(???)\")\n",
        "  y_ = model.predict(model_input)\n",
        "  predicted = y_[0].argsort()[-5:][::-1]\n",
        "  print(input, \"-->\",  \"expected :\", [vocab[i] for i in predicted])\n",
        "\n",
        "do_classify(\"연애상담하더니 둘이 사귀더라\", \"대화를 하다가 친해졌나봐요.\")\n",
        "do_classify(\"연애상담하더니 둘이 사귀더라\", \"대화를 하다가 친해졌나봐요.\")\n",
        "do_classify(\"연애상담하더니 둘이 사귀더라\", \"대화를 하다가 친해졌나봐요.\")\n",
        "do_classify(\"연애상담하더니 둘이 사귀더라\", \"대화를 하다가 친해졌나봐요.\")"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "연애상담(???) 둘이 사귀더라 대화를 하다가 친해졌나봐요. --> expected : ['##가', '##이', '안', '##에', '##의']\n",
            "연애상담하더니 둘이 사귀더라 대화를 하다가 친해졌나봐요(???) --> expected : ['.', '?', '!', '네', '##을']\n",
            "연애상담하더니 둘이 사귀더라 대화를 하다가 친해졌나(???). --> expected : ['##예요', '##해보세요', '##요', '##봐요', '##보세요']\n",
            "연애상담하더니 둘이 사귀더라 대화를(???) 친해졌나봐요. --> expected : ['있을', '생각', '될', '말', '해']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovKfa2IBZ-SZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ac50782-9725-4c62-e7a0-01b4a5f0aa48"
      },
      "source": [
        "do_classify(\"SNS 맞팔 왜 안하지ㅠㅠ\", \"잘 모르고 있을 수도 있어요.\")\n",
        "do_classify(\"SNS 맞팔 왜 안하지ㅠㅠ\", \"잘 모르고 있을 수도 있어요.\")\n",
        "do_classify(\"SNS 맞팔 왜 안하지ㅠㅠ\", \"잘 모르고 있을 수도 있어요.\")\n",
        "do_classify(\"SNS 맞팔 왜 안하지ㅠㅠ\", \"잘 모르고 있을 수도 있어요.\")\n",
        "do_classify(\"SNS 맞팔 왜 안하지ㅠㅠ\", \"잘 모르고 있을 수도 있어요.\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SNS 맞팔 왜 안하지ㅠㅠ 잘 모르고 있을 수도(???). --> expected : ['##예요', '##요', '##해보세요', '##보세요', '##봐요']\n",
            "SNS 맞팔 왜 안하지ㅠㅠ 잘 모르고 있을 수도 있어요(???) --> expected : ['.', '?', '!', '##을', '마음']\n",
            "SNS 맞팔 왜 안하지ㅠㅠ 잘 모르고(???) 수도 있어요. --> expected : ['거', '있을', '것', '생각', '될']\n",
            "SNS(???) 왜 안하지ㅠㅠ 잘 모르고 있을 수도 있어요. --> expected : ['##가', '##이', '##에', '안', '사람']\n",
            "SNS(???) 왜 안하지ㅠㅠ 잘 모르고 있을 수도 있어요. --> expected : ['##가', '##이', '##에', '안', '사람']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DibdK35ktIy9"
      },
      "source": [
        ""
      ],
      "execution_count": 51,
      "outputs": []
    }
  ]
}