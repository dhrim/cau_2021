{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /Users/mcpads/.miniforge3/envs/py38_tf26/lib/python3.8/site-packages (4.1.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/mcpads/.miniforge3/envs/py38_tf26/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/mcpads/.miniforge3/envs/py38_tf26/lib/python3.8/site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /Users/mcpads/.miniforge3/envs/py38_tf26/lib/python3.8/site-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 분류\n",
    "\n",
    "copied and modified from https://www.tensorflow.org/tutorials/keras/text_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 설정\n",
    "VOCA_SIZE = 4000 # 어휘 사전의 크기\n",
    "EMBEDDING_SIZE = 64 # 단어를 임베딩한 벡터 크기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 로딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_imdb_data(num_words=VOCA_SIZE):\n",
    "  print('Loading data...')\n",
    "\n",
    "  # 데이터\n",
    "  # (train_x, train_y), (test_x, test_y)\n",
    "  dataset = tf.keras.datasets.imdb.load_data(num_words=VOCA_SIZE)\n",
    "\n",
    "  # 단어와 정수 인덱스를 매핑한 딕셔너리\n",
    "  # word_index = {'fawn': 34701, 'tsukino': 52006, 'nunnery': 52007, ... }\n",
    "  word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "\n",
    "  return dataset, word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "((train_x, train_y), (test_x, test_y)), word_index = load_imdb_data()\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(train_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "<START> this film was just brilliant casting location scenery story direction <UNK> really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same <UNK> island as myself so i loved the fact there was a real connection with this film the witty <UNK> throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly <UNK> was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little <UNK> that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big <UNK> for the whole film but these children are amazing and should be <UNK> for what they have done don't you think the whole story was so lovely because it was true and was <UNK> life after all that was <UNK> with us all\n"
     ]
    }
   ],
   "source": [
    "# 처음 몇 개 인덱스는 사전에 정의되어 있습니다\n",
    "word_index = {k:(v+3) for k,v in word_index.items()}\n",
    "word_index[\"<PAD>\"] = 0\n",
    "word_index[\"<START>\"] = 1\n",
    "word_index[\"<UNK>\"] = 2  # unknown\n",
    "word_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "# reverse_word_index = {34704: 'fawn', 52009: 'tsukino', 52010: 'nunnery', ... }\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(train_x[0])\n",
    "print(decode_review(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각 데이터의 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n",
      "189\n",
      "141\n",
      "550\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "print(len(train_x[0]))\n",
    "print(len(train_x[1]))\n",
    "print(len(train_x[2]))\n",
    "print(len(train_x[3]))\n",
    "print(len(train_x[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 길이 일정하게 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "218\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(len(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 400)\n",
      "(25000, 400)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=400, padding='post')\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=400, padding='post')\n",
    "print(train_x.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1   14   22   16   43  530  973 1622 1385   65  458    2   66 3941\n",
      "    4  173   36  256    5   25  100   43  838  112   50  670    2    9\n",
      "   35  480  284    5  150    4  172  112  167    2  336  385   39    4\n",
      "  172    2 1111   17  546   38   13  447    4  192   50   16    6  147\n",
      " 2025   19   14   22    4 1920    2  469    4   22   71   87   12   16\n",
      "   43  530   38   76   15   13 1247    4   22   17  515   17   12   16\n",
      "  626   18    2    5   62  386   12    8  316    8  106    5    4 2223\n",
      "    2   16  480   66 3785   33    4  130   12   16   38  619    5   25\n",
      "  124   51   36  135   48   25 1415   33    6   22   12  215   28   77\n",
      "   52    5   14  407   16   82    2    8    4  107  117    2   15  256\n",
      "    4    2    7 3766    5  723   36   71   43  530  476   26  400  317\n",
      "   46    7    4    2 1029   13  104   88    4  381   15  297   98   32\n",
      " 2071   56   26  141    6  194    2   18    4  226   22   21  134  476\n",
      "   26  480    5  144   30    2   18   51   36   28  224   92   25  104\n",
      "    4  226   65   16   38 1334   88   12   16  283    5   16    2  113\n",
      "  103   32   15   16    2   19  178   32    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print(train_x[0])\n",
    "print(len(train_x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embedding 학습하며 예측 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gzip 파일 압축푸는 함수 (!gzip -d ---.gz)\n",
    "def download_and_decompress_gzip(http_link, target_link):\n",
    "  \"\"\"Utility function for download & decompress gzip file\"\"\"\n",
    "  import gzip, shutil, os\n",
    "  from urllib.request import urlretrieve\n",
    "\n",
    "  if os.path.exists(target_link): return\n",
    "\n",
    "  file_path, _ = urlretrieve(http_link)\n",
    "\n",
    "  with gzip.open(file_path, 'rb') as gz:\n",
    "    with open(target_link, 'wb') as f_out:\n",
    "        shutil.copyfileobj(gz, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3999\n"
     ]
    }
   ],
   "source": [
    "# 단어 목록을 계산\n",
    "def make_unique_word_set(*X):\n",
    "  word_set = set()\n",
    "\n",
    "  for dataset in X:\n",
    "    for sentence in dataset:\n",
    "      words = decode_review(sentence).split(\" \")\n",
    "      word_set.update(words)\n",
    "\n",
    "  print(len(word_set))\n",
    "  return word_set\n",
    "\n",
    "word_set = make_unique_word_set(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 외부 word2vec 모델 불러오는 함수\n",
    "\n",
    "def load_word2vec_model():\n",
    "  import gensim.downloader as api\n",
    "  return api.load('word2vec-google-news-300')\n",
    "\n",
    "def load_slim_word2vec_model():\n",
    "  from gensim.models import KeyedVectors\n",
    "\n",
    "  # 작은 모델을 로드해온다.\n",
    "  slim_model_link = \"https://github.com/eyaler/word2vec-slim/raw/master/GoogleNews-vectors-negative300-SLIM.bin.gz\"\n",
    "  model_path = 'GoogleNews-vectors-negative300-SLIM.bin'\n",
    "  download_and_decompress_gzip(slim_model_link, model_path)\n",
    "\n",
    "  return KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM\n",
    "\n",
    "# CNN Model\n",
    "def create_cnn_model(voca_size=VOCA_SIZE, embedding_size=EMBEDDING_SIZE):\n",
    "  model = Sequential()\n",
    "  model.add(Input(400))\n",
    "  model.add(Embedding(voca_size, embedding_size))  # 텍스트는 임베딩 해서 사용한다.\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Conv1D(250, 3))\n",
    "  model.add(GlobalMaxPooling1D())\n",
    "  model.add(Dense(250))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# RNN Model\n",
    "def create_rnn_model(voca_size=VOCA_SIZE, embedding_size=EMBEDDING_SIZE):\n",
    "  model = Sequential()\n",
    "  model.add(Input(400))\n",
    "  model.add(Embedding(voca_size, embedding_size))\n",
    "  model.add(Dropout(0.2))\n",
    "  # model.add(Conv1D(250, 3))\n",
    "  # model.add(GlobalMaxPooling1D())\n",
    "  model.add(Bidirectional(LSTM(64)))  # ADD\n",
    "  model.add(Dense(250))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "# RNN Model: From word2vec\n",
    "def create_rnn_model_from_word2vec(word2vec, voca_size=VOCA_SIZE):\n",
    "  embedding_size = word2vec.vector_size\n",
    "  embedding_matrix = np.zeros((voca_size, embedding_size))\n",
    "\n",
    "  # tokenizer에 있는 단어 사전을 순회하면서 word2vec의 300차원 vector를 가져옵니다\n",
    "  for idx, word in enumerate(word_set):\n",
    "      embedding_vector = word2vec[word] if word in word2vec else None\n",
    "      if embedding_vector is not None:\n",
    "          embedding_matrix[idx] = embedding_vector\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(Input(400))\n",
    "  model.add(Embedding(voca_size,\n",
    "                      embedding_size,\n",
    "                      input_length=400,\n",
    "                      weights=[embedding_matrix],\n",
    "                      trainable=False\n",
    "                      )\n",
    "            )\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Bidirectional(LSTM(64)))\n",
    "  model.add(Dense(250))\n",
    "  model.add(Dropout(0.2))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 13:24:55.501539: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2021-10-28 13:24:55.501621: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2021-10-28 13:24:55.697424: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-28 13:24:55.697604: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 400, 64)           256000    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               66048     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 250)               32250     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 251       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 354,549\n",
      "Trainable params: 354,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 13:24:56.452885: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-28 13:24:56.663167: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-28 13:24:56.682649: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-28 13:24:57.624793: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-28 13:24:57.638388: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - ETA: 0s - loss: 0.6860 - accuracy: 0.5482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-28 13:27:35.359131: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-28 13:27:35.403942: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2021-10-28 13:27:35.411632: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 219s 275ms/step - loss: 0.6860 - accuracy: 0.5482 - val_loss: 0.6565 - val_accuracy: 0.6139\n",
      "782/782 [==============================] - 60s 77ms/step - loss: 0.6565 - accuracy: 0.6139\n",
      "loss = 0.6564628481864929\n",
      "acc = 0.6138800382614136\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "\n",
    "# word2vec = load_word2vec_model()\n",
    "# word2vec = load_slim_word2vec_model()\n",
    "model = create_rnn_model() # create_rnn_model_from_word2vec(word2vec)\n",
    "model.summary()\n",
    "model.fit(train_x, train_y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(test_x, test_y))\n",
    "\n",
    "# Evaluation\n",
    "loss, acc = model.evaluate(test_x, test_y)\n",
    "print(\"loss =\", loss)\n",
    "print(\"acc =\", acc)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c33212796a18ce172644b359e00d0f030c7bb01214570476ad60cabee251e7cd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38_tf26': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
